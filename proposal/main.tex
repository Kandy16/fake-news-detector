\documentclass[a4paper, 11pt]{article}
\usepackage{a4wide}
\usepackage[ngerman,english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{ifthen}
\usepackage{bibgerm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{graphicx}
\usepackage{blindtext}

\topmargin 0cm \textheight 23cm \parindent0cm

% ---------------------------------------------
%	Commands definition
% ---------------------------------------------

\newcommand{\myName}{Kandhasamy Rajasekaran}
\newcommand{\emailID}{kandhasamy@uni-koblenz.de}
\newcommand{\matriculationID}{216100855}

\newcommand{\Title}{Fake News Detection using Neural Network models of Wikipedia}
\newcommand{\StartDate}{01.07.2018}
\newcommand{\EndDate}{31.12.2018}
\newcommand{\subject}{Institute for Web Science and Technologies}
\newcommand{\expert}{Prof. Dr. Steffen Staab}%inkl. Titel
\newcommand{\supervisor}{Supervisor????} %inkl. Titel
\newcommand{\scndSupervisor}{Lukas Schmelzeisen} %inkl. Titel
\newcommand{\type}{Master Thesis}

\newcommand{\enhanced}{Enhanced Stitched-Viewport Screenshot}
\newcommand{\stitched}{Stitched-Viewport Screenshot}
\newcommand{\expanded}{Expanded-Viewport Screenshot}


\begin{document}
% ---------------------------------------------
%	Title
% ---------------------------------------------
\selectlanguage{ngerman}
Universit\"{a}t Koblenz - Landau \hfill \today

Department f\"{u}r Informatik,

\subject{}

\expert{}

\supervisor{}

\scndSupervisor{}

\begin{center}
	\large{\bf \type{}  \myName{}}

	\vspace*{0.5cm}

	\large{\bf \Title}
\end{center}

\setlength{\parskip}{1.5ex plus0.5ex minus 0.5ex}
% -----------------------------------------------------------------------------
%	Content
% -----------------------------------------------------------------------------
\selectlanguage{english}
\begin{abstract}
\frenchspacing
\noindent
With the unprecedented growth of production and dissemination of information, there exists an unprecedented growth in production and dissemination of fake news. There have been several adverse incidents that happened in the past due to rise of fake news. It is very essential to have a mechanism to detect and control fake news automatically. Several methods have been proposed which are automatic and semi automatic than the traditional manual checking systems. In this master thesis project, an attempt is to be made to understand online encyclopedias using deep learning technique. The performance of the system will be benchmarked against the results that have been published for those systems.
\end{abstract}
% -----------------------------------------------------------------------------
\section{Introduction}
\frenchspacing

%News/Information - how important it is to the society, how society advances
%Fake news - definition(Citation?)
% Fake news in web -  its prevalence and the disadvantages (Citation - science magazine paper). Disadvantage example from the paper?

%Fake news detection template - Compare against reliable source of information - that is the way to detect it, experts from each domain give their views and a consolidated decision (Citation?) Systems such as factcheck, snopes etc How they do it as against the rate at which fake news is generated?

%Manual correction is not possible. Web has seen an unprecedented growth. It is important to have automated means. (Citation?)

%Wikipedia - say 1 or 2 sentences about it being a source of information - different subject matters - wide coverage. Frequency of updates (Citation?)
%About the reliability of source of information in wikipedia (Citation?) - Wikipedia is peer reviewed as against the newspaper

%Deeplearning - what is it? Advantages and how is it performing against others? How NLP problems are handled using RNN? What applications?

%Fake news detection using wikipedia as a ground reality

Humans evolved and became a superior race than other species only by being able to pass the information to its descendants. The society advances as a whole by improving its collective intelligence and that happens only by sharing. With the advent of technology systems such as web, it is possible for anyone to become a publisher and publish their thoughts, ideas to the entire world. As much as it became easier to disseminate information it also brought in a lot of negative effects. One of it is the spread of false news or fake news. The dissemination of false news can harm the society very bad and it can reduce the progress or worsen the state of the society. There are several examples such as a misleading news refering to a fire in a US school made a lot of chaos in traffic and accidents because parents were rushing to save their kids. Humans have been relying on their collective intelligence and advice from experts on a particular domain to handle the fake news. There are several organizations such as snopes, factcheck.org, politfact which tags many information with a fact meter to label the authenticity of information. Although this is fairly good, it is not scalable and unmatchable to the rate at which the information is created. With many social networks, blogging sites we have seen a unparallel rise in the volume of content being generated and a manual intervention to cross check their authenticity is clearly no match. But overall the main idea to handle false news is to check against reliable source of information and claim its integrity.

Wikipedia is a crowdsourced online encylopedia which has versatile topics and a whole range of articles for each topic. It has been listed as one of the famous 10 websites or the one of the 10 most used website. The range of subject covered is very wide and it comes in different languages. The frequency of the udpate is really high and every content is peer reviewed by other as against the process in newspaper. Although the checking need not be done by experts opinion the reliability of common public on wikipeida is highly visible. 

The focus of the master thesis is to use wikipedia a ground reality or as source of experts opinion and use this knowledge to cross check the claims which in the form of tweets, blogs, sentences through automated means. Recurrent Neural Network are a special kind of neural network models for sequence data. They are used to understand wikipedia and acts a bot to check the news as fake or not. In this we assume whatever information present in wikipedia is truthy and we rely on the collective and collaborative effort of humans to build this enormous information resource.
% ----------------------------------------------------------------------------
\section{Related work}

There are many attempts being made to counter attack fake news. Most of them are imbibed into one particular platform and uses the news, user characteristics, who shared it and what is their credibility and how the diffusion have happened. They used many supervised machine learning algorithms which uses handcrafted features and obtain substantial results. 

Giovanni Luca Ciampaglia et al. used DBPedia for checking computationally whether a given information is factual or not. The work involves using the knowledge graph built from DBPedia which represents infobox section in Wikipedia. This represents only non-controversial and factual information which is analagous to human collected information. The methodology formulates the problem of checking facts into a network analysis problem which is finding the shortest path between nodes (subject and object of a sentence) in a graph. The aggregated generalities of nodes along a path in a weighted undirected graph is used as a metric for measuring the authenticity of information. The more the elements are generic the weaker the truthfulness is.  The genericness of a node is obtained by the degree of that node - no. of nodes connected to that node. The truthfulness of the information is improved if there exists at least one path from subject to an object with minimal non-generic nodes. This approach exploits the indirect connections to a great extent with distance constraints in a knowledge graph. The approach gave promising results when tested with datasets containing simple factual information about history, geography, entertainment and biography. 

%limitations
%-----------
%1)primitive - the complexity of news dealt with is very primitive. Current fake news are very complex and subtle when it comes to the ambiquities
%2) Semantic proximity methodology is simple and primitive


%But it is a good initial step towards an automated fact checker system.


%What improvements are we going to bring?
%----------------------------------------
%1) Ours would also be primitive :-) May be slightly better :-)
%2) Frequency of conversion of wikipedia content into DBPedia is lesser ??
%3) DBPedia does not cover the whole of wikipedia ???? 

%Things to do:
%-------------
%1) Find the answers for above questions
%2) Get the dataset that we can use?
%3) Try to look for a demo of this system

Jing Ma et al. efforts were focused on building a recurrent neural network (RNN) to detect rumors from Microblogs effectively. The social context information of a post and all its relevant posts such as comments or retweets is modeled as variable-length time series.  RNNs with different configurations such as using one or two layers of GRU and LSTM are very good in capturing long distance dependencies of temporal and textual representations of posts under supervision. This method completely avoids all the handcrafted feature engineering efforts which are biased and time consuming. It produces better results with datasets from Twitter and Sina Weibo than all of the traditional Machine Learning methods. RNNs with two layers of GRU gave the best results and it was also very quick in predicting the rumor than the average time from debunking services.  

%Attempts made to figure out fake news in the past
%Manually curated facts dataset
%If there are any automatic ones
%It would be nice to see a forum/wiki (I hope not wikipedia :-)) being used and the methodology that have been used

%Research by Filippo Menczer - uses the DBPedia which is a structured wikipedia and use it to cross check claims
%https://www.mendeley.com/viewer/?fileId=631c34ce-5e90-722b-2974-6d71a44ad9ef&documentId=4c0051e0-64fb-3e10-959e-ac8046c9002b

%Twitter has been used - 
%https://885d47c6-a-62cb3a1a-s-sites.googlegroups.com/site/iswgao/home/ijcai16.pdf?attachauth=ANoY7cq8X9N6sGSW7wLfGloRItBkaFO1a2ELhv4s2rWN8VXGMbTwuTjUh_uGRA5vslyvOT1UDNx5wpxCWdZLNeaBcqvLO9N3dfgJfhphfDNv3pZh1P69EgHWJZeg2wGjSGDI-bhBo4VHDDwFqqM-JDoNCigNHEoTK3zDi4Dn6mGIAMcmOUfs6KrEBdAk0QUpJWPmrARCylfQe41FkVkZ0Hkpo2w-akFauQ%3D%3D&attredirects=0

%Social networks - using usage patterns in social networks- 
%https://www.mendeley.com/viewer/?fileId=6e662e87-8846-c9c3-5a3b-505f835bd89b&documentId=f238c7a0-21a1-3cab-9ed4-1c864aa0011c

% ----------------------------------------------------------------------------
\section{Approach}

Deep learning systems are giving better results in building intellectual systems nowadays.\cite{Goldberg2016} The results achieved in applications such as Autonomous car driving, playing chess are almost equivalent to the skillsets of a human. There are different neural network models exists such as Feed Forward, Convolutional, Recurrent Neural Network models. Wikipedia contains a lot of articles and each articles contains text. Text is a sequence data where as position of words in it is dependent on the other. Recurrent Neural Networks bring out best results in many applications involving text data such as machine translation, super tagging.

Every article or sentence in wikipedia can be fetched and inputted to the neural network as truthy value. But we lack false values. We need to use a semi supervised technique where in extraction of falsy values need to be carried out automatically. This is one of the challenges of this master thesis. 

Some of the ideas in place are
1) Extract the articles and sentences from many of these fact checkers websites and use the ones which are labelled as false values. We need to make sure that they are opposites of sentences in wikipedia.
2) Build sentences which are opposite of sentences in wikipedia by using GLOVE technique or construct negative sentences. 
3) Build sentences which are opposite of sentences in wikipedia by looking a semantic web representations. Word embeddings can be used to replace the verb opposites 

The size of the article in wikipedia are long and arbitrary and RNN will face gradient diminishing problems. The long distance dependencies will be missed out and hence different configurations of the neural network should be used and compared 
1) Different layers
2) Single/Multiple LSTM or GRU units
3) Usage of one hot vector vs word embeddings. The creation of word embeddings need to be thought through. It would be a good idea to do it from either wikipedia itself or from pre-trained word embeddiings


Recently convolutional neural network which character level input is giving out better results for some applications and this configuraiton should also be tried out.

%Deep learning - using RNN network architecture for sequential models
%One hot representation or word vector
%https://www.mendeley.com/viewer/?fileId=3eea8d80-58e9-51f5-5b45-945420ba7577&documentId=a45b2cdb-dae1-3d54-8d47-395580293df7

%Usage of LSTM - to support long range dependencies
%Wordvectors? 
%Language modeling? - Based on the probability of occurrence - we can predict how likely the sentence can occur or predict the next word. Given a news - predict how likely it can happen. But this may  not give a good result - so, can be used as a baseline

%Some research questions are

%What would be the best word vector representation for this problem - how do we prepare it? - have to generalize it
%How do we create negative examples? What is the best way to do it for this kind of problem?
%What lies can be detected effectively? This will explain what level of information is updated in wikipedia in each domain - constructing the dataset would be tricky. Can look at ways how to construct datasets based on the domain


% ----------------------------------------------------------------------------
\section{Evaluation}

The training, evaluation and test dataset needs to be curated
Enough data should be present for each bin such as facts, slightly distorted lie, blatant lies 

Extract sentence from wikipedia and give it as it is
Distort the sentence by swapping and give it 

Construct good facts and lies outside - a proper labelled dataset and see how it works
\newpage

% ----------------------------------------------------------------------------
\section{Organizational matters}

\begin{tabbing}
Duration of work: \hspace{1.1cm} \= \StartDate{} -- \EndDate{}\\
\vspace{0.5ex}Candidate:	\> \myName{}\\
\vspace{0.5ex}E-Mail:	\> \emailID{}\\
\vspace{0.5ex}Student number: \> \matriculationID{}\\
\vspace{0.5ex}Primary supervisor: \> \expert{}\\
Supervisor: \> \supervisor{}\\
Secondary supervisor: \> \scndSupervisor{}\\
\end{tabbing}

% ----------------------------------------------------------------------------

\section{Time schedule}

This needs to be changed. But for now just a filler.

\begin{itemize}
	\item Introduction and Literature: 03.05.17 – 15.06.17
	\item Methodology: 09.06.17 – 30.07.17
	\begin{itemize}
		\item Approach concept: 09.06.17 – 22.06.17
		\item Implementation Plan: 18.06.17 – 29.06.17
        \item Implementation: 29.06.17 – 30.07.17
        \item Testing and refining: 01.07.17 – 30.07.17
	\end{itemize}
	\item Approach results: 31.07.17 – 20.08.17
	\begin{itemize}
		\item Sampling: 31.07.17 – 12.08.17
		\item Interpretation: 13.08.17 – 20.08.17
	\end{itemize}
	\item Evaluation: 21.08.17 – 25.09.17
	\begin{itemize}
		\item Preparing evaluation: 21.08.17 – 01.09.17
		\item Conducting evaluation: 02.09.17 – 11.09.17
		\item Analyzing results: 12.09.17 – 25.09.17
	\end{itemize}
	\item Revision: 26.09.17 – 31.10.17
\end{itemize}


% ----------------------------------------------------------------------------
\bibliographystyle{alpha}
\bibliography{bib}
\newpage
% ----------------------------------------------------------------------------
\section{Signatures}

\vspace{3cm}
\begin{tabular}{ccc}
  --------------------------------------------------- &  & ---------------------------------------------------\\
  \myName{} &  & \expert{}  \\ \vspace{3cm}
   &  &   \\
  --------------------------------------------------- &  & ---------------------------------------------------\\
  \supervisor{} &  & \scndSupervisor{}  \\ \vspace{3cm}
   &  &   \\
\end{tabular}

\newpage
% ----------------------------------------------------------------------------
\section{Declaration of Authorship}
I hereby declare that the thesis submitted is my own unaided work. All direct or indirect sources used are acknowledged as references.

I am aware that the thesis in digital form can be examined for the use of unauthorized aid and in order to determine whether the thesis as a whole or parts incorporated in it may be deemed as plagiarism. For the comparison of my work with existing sources I agree that it shall be entered in a database where it shall also remain after examination, to enable comparison with future theses submitted. Further rights of reproduction and usage, however, are not granted here.

This paper was not previously presented to another examination board and has not been published.

\vspace{3cm}
\begin{tabular}{ccc}

  Koblenz, on \today &  &  \\
     &  & ---------------------------------------------------\\
   &  & \myName{}  \\
\end{tabular}



\end{document}









