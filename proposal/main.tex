\documentclass[a4paper, 11pt]{article}
\usepackage{a4wide}
\usepackage[ngerman,english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{ifthen}
\usepackage{bibgerm}
\usepackage{graphicx}
\graphicspath{{./images/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.svg}
\usepackage{hyperref}
\usepackage{movie15}
\usepackage{fixltx2e}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{color}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{color}
\topmargin 0cm \textheight 23cm \parindent0cm

% ---------------------------------------------
%	Commands definition
% ---------------------------------------------
\newcommand{\R}{\mathbb{R}}

\newcommand{\myName}{Kandhasamy Rajasekaran}
\newcommand{\emailID}{kandhasamy@uni-koblenz.de}
\newcommand{\matriculationID}{216100855}

\newcommand{\Title}{Fake news classification through Wikipedia using recurrent neural networks}
\newcommand{\StartDate}{01-July-2018}
\newcommand{\EndDate}{31-Dec-2018}
\newcommand{\subject}{Institute for Web Science and Technologies}
\newcommand{\expert}{Prof. Dr. Steffen Staab}%inkl. Titel
\newcommand{\supervisor}{Dr. Chandan Kumar} %inkl. Titel
\newcommand{\secondSupervisor}{Lukas Schmelzeisen} %inkl. Titel
\newcommand{\type}{Master Thesis}
\begin{document}
% ---------------------------------------------
%	Title
% ---------------------------------------------
\selectlanguage{ngerman}
Universit\"{a}t Koblenz - Landau \hfill \today

Department f\"{u}r Informatik,

\subject{}

\expert{}

\supervisor{}

\secondSupervisor{}

\begin{center}
	\large{\bf \type{}  \myName{}}

	\vspace*{0.5cm}

	\large{\bf \Title}
\end{center}

\setlength{\parskip}{1.5ex plus0.5ex minus 0.5ex}
% -----------------------------------------------------------------------------
%	Content
% -----------------------------------------------------------------------------
\selectlanguage{english}
%\begin{abstract}
%\frenchspacing
%\noindent
%The unprecedented growth in production and dissemination of information leads to an unprecedented growth in production and dissemination of fake news. Fake news hinders the society from progress by deterring the pursuit of true information. It is very essential to have a mechanism to detect and control fake news. Several attempts have been made which uses platform specific features, collaborative efforts of domain experts to control fake news. This research work will use wikipedia as a ground reality and cross check claims automatically. Recurrent neural networks will be used to understand wikipedia along with multiple novel ways to build false news. The performance of different configurations of neural networks will be analyzed and benchmarked against each other.
%\end{abstract}
% -----------------------------------------------------------------------------
\section{Introduction}
\frenchspacing

%Fake news - definition(Citation?) and prevalence
According to Lazer et al.\cite{Lazer1094}, Fake news is a false information, constructed intentionally (disinformation) or unintentionally (misinformation) and the publishers does not have rigorous news media's editorial norms for making sure of accuracy and credibility. The focus of this master thesis is to classify a claim as true or fake news by using Wikipedia as a ground reality. In this thesis work, the information in Wikipedia will be captured using different approaches and will be evaluated based on how good it classifies fake news.

Fake news is prevalent and a research study from Allcot and Gentzkow \cite{Allcott2017} state that "average American adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them". A large-scale empirical study with twitter dataset by Vosoughi et al. \cite{Vosoughi1146}, reveals that fake news spread longer, faster, deeper and broader than the legitimate news. It also reveals that the effects of fake news are more prominent in a political context than news about terrorism, natural disaster, science and other domains.

%Fake news detection template - Compare against reliable source of information - that is the way to detect it, experts from each domain give their views and a consolidated decision (Citation?) Systems such as factcheck, snopes etc How they do it as against the rate at which fake news is generated?. %Manual correction is not possible. Web has seen an unprecedented growth. It is important to have automated means. (Citation?)
Most often fake news is detected by people and organizations through their common sense. There are many facts checking websites such as for e.g. snopes.com, factcheck.org, politifact.com etc. which uses collaborative effort of domain experts. In these websites, each news is tagged with a fact meter to refer its authenticity. The main strategy to handle fake news is to check against a reliable source of information and claim its integrity. Although this is fairly good, since it requires manual effort, it is not available for all domains. Also, it is unmatchable to the rate at which the information is produced because of many social networks, blogging sites etc.

%Wikipedia - say 1 or 2 sentences about it being a source of information - different subject matters - wide coverage. Frequency of updates (Citation?). About the reliability of source of information in wikipedia (Citation?) - Wikipedia is peer reviewed as against the newspaper
Wikipedia is a free online encyclopedia available in more than 300 languages with a principle that anyone can edit \cite{Wales2005}. It covers a wide range of domains whereas many articles are available in each domain. According to Alexa\footnote{https://www.alexa.com} and SimilarWeb\footnote{https://www.similarweb.com}, Wikipedia is considered to be the fifth most popular website. According to Wikipedia in 2018, the English Wikipedia consists the highest number of articles amounting to approximately 500 million. Also, the frequency of the updates in English Wikipedia is very high and it is approximately equal to 10 updates per second and 600 articles per day. According to a research by Halavais et al. \cite{Halavais2008}, it is estimated that domains such as music, geography, science and literature constitute atleast 5 percent of total articles each in Wikipedia. Although it can be edited by anyone, an investigation carried out by Nature\footnote{https://www.nature.com}, reveals that the quality of content is similar to another encyclopedia such as Britannica\footnote{https://www.britannica.com/} \cite{Wales2005}. Although there can be malicious users in Wikipedia, the culture and the community ensures most of the high impactful errors are rectified very quickly\cite{Priedhorsky2007}. Thus English Wikipedia can be used as a proxy for a reliable source of information since most of its content is true.
 
%Fake news detection using wikipedia as a ground reality (Attempt in Master thesis)
In this master thesis, the Wikipedia will be considered as a ground reality or as a source of experts opinion and this knowledge will be used to cross-check claims automatically. The presence or absence of information in Wikipedia will be used as a proxy for information being considered as true or fake. Recurrent neural networks with different configurations will be used to understand Wikipedia and the results of each will be compared against each other to understand better.

The "Related work" section consists a brief summary of different researches done in the past which are relevant to fake news classification using semantics and platform specific features. An introduction of neural networks, different architectures of neural networks to model sequence data and the training process involved to optimize the model parameter values are covered in "Background study" section. The "Approach" section comprises the overall work to be done in the thesis which includes different techniques of modeling the information in Wikipedia to build a good fake news classifier. The "Evaluation" section describes what factors will be considered for evaluting the classifier and what standard test datasets will be used to benchmark against each other.
% ----------------------------------------------------------------------------
\section{Related work}

% FIND PAPERS FOR FOLLOWING
% Manual method for fake news - people checking sources manually in politfact, factcheck etc
% Platform based methods - manual/semi automated /automated feature engineering efforts
% Checking the authenticity semantically - using RDF, wikipedia

Many efforts in research have been put in to detect fake news in microblogging platforms such as Twitter\footnote{https://twitter.com/}. Most of these works classify the news as truth or fake by using the platform/user-specific information such as how popular the post is, the credibility of the user who shared it, diffusion patterns etc \cite{Liu2015} \cite{Ma2015}. Zhao, et al. \cite{Zhao2015} have used cue terms such as 'not true', 'unconfirmed' etc in retweets or the comments to detect fake news. The reasoning in their approach is that when people exposed to fake news they will comment or retweet with such words as a response to the post. Other studies focused on using the temporal characteristics of fake news during the spread. Kwon et al.\cite{Kwon2013} used tweet volume in time series and Ma, et al.\cite{Ma2015} measured variations of social context features over time. 

All the research indicated before used datasets which are smaller in size. Wang \cite{Wang2017} curated dataset which consists approximately 13000 short statements by mining politfact.com covering a decade of information. In this approach, six machine learning models are built ranging from logistic regression to the convolutional neural network and compared. Along with the text data, metadata such as the speaker, subject, speaker history are also used. The convolutional neural network model used to capture surface level linguistics along with metadata performed better than other models.

All the attempts made in the above researches involve handcrafted feature engineering which is critical, biased and very time-consuming.

Jing Ma et al.\cite{Ma} efforts are focused on building a recurrent neural network (RNN) to detect rumors from Microblogs such as Twitter and Weibo\footnote{https://www.weibo.com} effectively. The training dataset used is obtained by using constructed keywords in fake and true news from debunking services such as Snopes\footnote{https://www.snopes.com/} and Sina community management center\footnote{https://www.sina.com.cn/}. These keywords are used in Search API's of Microblogs and the tweet results of a search are labeled respectively. The social context information of a post and all its relevant posts such as comments or retweets is modeled as a variable-length time series. RNNs with different configurations such as one or two layers of GRU and LSTM are used and achieved very good results in capturing long-distance dependencies of temporal and textual representations of posts under supervision. This method completely avoids all the handcrafted feature engineering efforts which are biased and time-consuming. It produces better results with datasets from Twitter and Sina Weibo than all of the traditional Machine Learning methods. RNNs with two layers of GRU gave the best results and it was also very quick in predicting the rumor than the average time from debunking services.

The above method uses platform-specific features immensely such as retweets, comments in a tweet and the temporal correlation between them to figure out whether the news is true or fake. The features specified in one platform will be different from the other and it is not guaranteed that this methodology will give the same results for the same news in two different platforms. The semantics of the tweets are not used and if used then it would remain the same across platforms. 

Ciampaglia et al. \cite{Ciampaglia2015} used DBPedia\footnote{https://wiki.dbpedia.org/} for checking computationally whether a given information is factual or not. The work uses the knowledge graph built from DBPedia which represents infobox section in Wikipedia. This represents only non-controversial and factual information which is analogous to human collected information. The methodology formulates the problem of checking facts into a network analysis problem which is finding the shortest path between nodes (subject and object of a sentence) in a graph. The aggregated generalities of nodes along a path in a weighted undirected graph are used as a metric for measuring the authenticity of information. The more the elements are generic; the weaker the truthfulness is.  The genericness of a node is obtained by the degree of that node - no. of nodes connected to that node. The truthfulness of the information is improved if there exists at least one path from subject to an object with minimal non-generic nodes. This approach exploits the indirect connections to a great extent with distance constraints in a knowledge graph. The approach gave promising results when tested with datasets containing simple factual information about history, geography, entertainment, and biography. 

The above research is a good initial step towards an automated fact checker system using only the semantics of data. The problem of fake news attempted is very primitive and uses only 'is' or 'type of' relation. The current fake news is very complex and subtle when it comes to ambiguities. In this approach, DBPedia is used and according to their sources, the update/synch frequency is slower than Wikipedia by 6 to 18 months.

%limitations
%-----------
%1)primitive - the complexity of news dealt with is very primitive. Current fake news are very complex and subtle when it comes to the ambiquities
%2) Semantic proximity methodology is simple and primitive


%But it is a good initial step towards an automated fact checker system.


%What improvements are we going to bring?
%----------------------------------------
%1) Ours would also be primitive :-) May be slightly better :-)
%2) Frequency of conversion of wikipedia content into DBPedia is lesser ??
%3) DBPedia does not cover the whole of wikipedia ???? 

%Things to do:
%-------------
%1) Find the answers for above questions
%2) Get the dataset that we can use?
%3) Try to look for a demo of this system

  

%Attempts made to figure out fake news in the past
%Manually curated facts dataset
%If there are any automatic ones
%It would be nice to see a forum/wiki (I hope not wikipedia :-)) being used and the methodology that have been used

%Research by Filippo Menczer - uses the DBPedia which is a structured wikipedia and use it to cross check claims
%https://www.mendeley.com/viewer/?fileId=631c34ce-5e90-722b-2974-6d71a44ad9ef&documentId=4c0051e0-64fb-3e10-959e-ac8046c9002b

%Twitter has been used - 
%https://885d47c6-a-62cb3a1a-s-sites.googlegroups.com/site/iswgao/home/ijcai16.pdf?attachauth=ANoY7cq8X9N6sGSW7wLfGloRItBkaFO1a2ELhv4s2rWN8VXGMbTwuTjUh_uGRA5vslyvOT1UDNx5wpxCWdZLNeaBcqvLO9N3dfgJfhphfDNv3pZh1P69EgHWJZeg2wGjSGDI-bhBo4VHDDwFqqM-JDoNCigNHEoTK3zDi4Dn6mGIAMcmOUfs6KrEBdAk0QUpJWPmrARCylfQe41FkVkZ0Hkpo2w-akFauQ%3D%3D&attredirects=0

%Social networks - using usage patterns in social networks- 
%https://www.mendeley.com/viewer/?fileId=6e662e87-8846-c9c3-5a3b-505f835bd89b&documentId=f238c7a0-21a1-3cab-9ed4-1c864aa0011c

% ----------------------------------------------------------------------------
\section{Background Study}

Neural networks are state of the art models to build learning systems. In the beginning, neural networks were inspired by the brain's computational mechanism \cite{McCulloch1943}. Neural networks compose many interconnected, fundamental, functional units called neurons. Each neuron in the network takes in multiple scalar inputs and multiplies each input by a weight and then sums them, adds the result with a bias, applies a non-linear function at the end, which gives out a scalar output. There are different architectures of neural networks which vary mostly in how the neurons are connected to each other and how the weights are managed. 

Feedforward neural networks \cite{Svozil1997} can have multiple layers and each neuron in one layer is connected with every other neuron in the subsequent layer as given in the Figure \ref{fig:Feed forward neural network}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth,height=8cm,keepaspectratio=true]
    {feed-forward-neural-network.png}
    \caption{
        A Feed forward neural network.
    }
    \label{fig:Feed forward neural network}
\end{figure}

\begin{equation}
\mathrm{NN_{MLP1}}(x) = y
\end{equation}
\begin{equation} \label{equ: feed forward hidden layer}
h^1 = g(xW^1 + b^1)
\end{equation}
\begin{equation} \label{equ: feed forward output layer}
y = h^1W^2 + b^2
\end{equation}
\begin{align*}
x \in \R^{d_{in}}, W^1 \in \R^{d_{in}*d_1}, b^1 \in \R^{d_1}, W^2 \in \R^{d_1*d_2}, b^2 \in \R^{d_2}
\end{align*}

Here W\textsuperscript{1} and b\textsuperscript{1} are a matrix and a bias term for the first linear transformation of input, g is a non-linear function that is applied element-wise, and W\textsuperscript{2} and b\textsuperscript{2} are matrix and bias term for second linear transform. With respect to the figure \ref{fig:Feed forward neural network}, the values of d\textsubscript{in}, d\textsubscript{1} and d\textsubscript{2} are 3, 4 and 2 respectively. 

There are 3 layers in the figure \ref{fig:Feed forward neural network}. Each circle is a neuron with incoming lines as inputs and outgoing lines as outputs to the next layer. Each line carries a weight and the input layer has no weights since it has no incoming lines. The input layer consists of 3 neurons and the extracted features of raw data will be sent through these neurons. The hidden layer consists of 4 neurons, where as each neuron takes 3 inputs from input layer. Each input is multiplied with a unique weight variable and added together. Finally, the output is added with 1 bias variable and will be passed to a non-linear activation function as shown in equation \ref{equ: feed forward hidden layer}. The activation function help the neural network models to approximate any nonlinear function. Different activation functions pose different advantages. Some activation functions that will be used in this thesis are sigmoid, hyperbolic tangent and rectifiers \cite{Goldberg2016}. Rectifiers are used most commonly.

The sigmoid activation function is a S-shaped function which transforms any value between the range 0 and 1. This is considered to be depreceated since other functions have been giving better results empirically. 

\begin{align*}
\sigma (x) = \frac{1}{1 + e^{-x}}
\end{align*} 

The hyperbolic tangent function is also a S-shaped function, but it transforms any value between the range -1 and 1.

\begin{align*}
tanh(x) = \frac{e^{2x}-1}{e^{2x}+1}
\end{align*}

The Rectifier activation function clips values lesser than 0 and it performs faster and better than sigmoid and hybolic tangent functions.

\begin{align*}
ReLU(x) = max(0,x)
\end{align*}

The output layer consists 2 neurons and each will take 3 inputs from hidden layer as shown in equation \ref{equ: feed forward output layer}. If it has no outgoing lines then it will be used as final output. The output layer can use a transformation function such as softmax to convert values to represent a discrete probability distribution. In this figure, 2 neurons are used to refer to 2 labels and this system classifies the input into one of those labels. 

\begin{align*}
y = y_1,y_2...,y_k \\
s_i = softmax(y_i) = \frac{e^{y_i}}{(\sum_{j=1}^ke^{y_j})}
\end{align*}

Training is an essential part of learning and like many supervised algorithms, a loss function is used to compute the error for the predicted output against the actual output. Some of the loss functions that could be used are hinge loss (binary and multiclass), log loss, categorical cross-entropy loss etc \cite{Goldberg2016}. 

\begin{align*}
L_{hinge(binary)}(\hat{y},y) = max(0,1-y.\hat{y})
\end{align*}

\begin{align*}
L_{cross-entropy}(\hat{y},y) = -\sum_iy_i.log(\hat{y_i}) \\
L_{cross-entropy(hard classification)}(\hat{y},y) = -log(\hat{y_t})
\end{align*}

The gradient of the errors is calculated and propagated back to compute with respect to weights and bias. The values of the weights and bias are adjusted with respect to the gradient and a learning parameter. Typically a random batch of inputs is selected and a forward pass is carried out which involves multiplying weights, adding bias and applying a activation function on top of it to predict outputs. The average loss is computed for that batch and the parameters are adjusted accordingly. This optimization technique is called stochastic gradient descent \cite{Bottou2012}. A number of extensions exists, such as Nesterov Momentum \cite{Sutskever2013} or AdaGrad \cite{Duchi2011}. The overfitting in neural networks can be minimized by using regularization techniques such as L\textsubscript{2} regularization and dropout \cite{Hinton2012}. The L\textsubscript{2} regularization works by adding a penalty term equal to sum of the squares of all the parameters in the network to the loss function which is being minimized. The dropout works by randomly ignoring half of the neurons in every layer in each batch and corrects the error only using the parameters of other half of neurons. This helps to prevent the network from relying on only specific weights. 

Feedforward networks work very well on structured input data but, in case of text data, the input is sequential. Techniques such as continuous bag of words \cite{DBLP:journals/corr/abs-1301-3781} can be used to convert sequential input into fixed length but it will lose the order of the text information which is important. Convolutional neural networks (CNN) \cite{Bengio1997} are good at capturing the local characteristics of data irrespective of its position. In this, a nonlinear function is applied to every k-word sliding window and captures the important characteristics of the word in that window. All the important characteristics from each window are combined by either taking maximum or average value from each window. This captures the important characteristics of a sentence irrespective of its location. However, because of the nature of CNNs they fail to recognize patterns that are far apart in the sequence.

Recurrent neural networks (RNN) accept sequential inputs and are often able to extract patterns over long distances \cite{Elman}. RNN takes input as an ordered list of input vectors such as x\textsubscript{i:j} with initial state vector h\textsubscript{0} and returns an ordered list of state vectors h\textsubscript{1},...,h\textsubscript{n} as well as an ordered list of output vectors o\textsubscript{1},...,o\textsubscript{n}. At time step t, a RNN takes as input a state vector h\textsubscript{t-1}, an input vector x\textsubscript{t} and outputs a new state vector h\textsubscript{t} as shown in the figure \ref{fig:A basic RNN architecture}. The outputted state vector is used as input state vector at the next time step. The same weights for input, state, and output vectors are used in each time step.  

%RNN - architecture, how does it support the sequence, (Elman, 1990) 
%Back Propagation through time (BPTT) - (Werbos, 1990) \cite{Werbos1990}
%Long distance dependencies - LSTM and GRU (Hochreiter and Schmidhuber 1997) and (Cho et al.)
%(did not find the citation for GRU) \cite{Hochreiter1997}
%Recursive Neural Networks - for syntactic structure (trees ) (pollack 1990; socher, manning and ng 2010) \cite{Pollack1990} \cite{Socher}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth,height=6cm,keepaspectratio=true]
    {Recurrent_neural_network_unfold.png}
    \caption{
        A basic example of RNN architecture \cite{WikipediaEN_RNN_unfold}.
    }
    \label{fig:A basic RNN architecture}
\end{figure}

\begin{align*}
\mathrm{RNN}(s_0,x_{1:n}) = s_{1:n}, y_{1:n} \\
s_i = R(s_{i-1},x_i) \\
y_i = O(s_i) \\
x_i \in \R^{d_{in}}, y_i \in \R^{d_{out}}, s_i \in \R^{f(d_{out})} 
\end{align*}

To train a RNN, the network is unrolled for the given input sequence and the loss function is used to compute the gradient of error with respect to parameters involved in every time step by propagating backwards through time. After that the parameters are adjusted to reduce the error in estimation\cite{Werbos1990}. While training, the error gradients might vanish or explode especially when dealing with RNNs. The gradient explosion can be handled by clipping the gradient when it goes beyond the threshold. LSTM networks \cite{Hochreiter1997} solves the vanishing gradient problem by introducing memory cells which remembers gradients across time steps. These memory cells are controlled by gating components which decides at every time step, on what parts of the hidden state should be carried over and what parts of new input should be included. 

\begin{align*}
s_j = R_{LSTM}(s_{j-1},x_j) = [c_j;h_j] \\
c_j = c_{j-1} \odot f + g \odot i \\
h_j = tanh(c_j) \odot o \\
i = \sigma(x_jW^{xi} + h_{j-1} W^{hi}) \\
f = \sigma(x_jW^{xf} + h_{j-1} W^{hf}) \\
o = \sigma(x_jW^{xo} + h_{j-1} W^{ho}) \\
g = \sigma(x_jW^{xg} + h_{j-1} W^{hg}) \\
\\
y_j = O_{LSTM}(s_j) = h_j \\
\\
s_j \in \R^{2.d_h},   x_i \in \R^{d_x},  c_j,h_j,i,f,o,g \in \R^d_h,  W^{xo} \in \R^{d_x*d_h},  W^{ho} \in \R^{d_h*d_h}
\end{align*}

\section{Approach} 

You should rewrite this complete section. Start with the general architecture diagram, that we talked about in the last meeting (i.e. a black box taking a sentence and a context as input and outputting a binary output). Then discuss the different  context definitions that we plan to compare. In the following roughly scetch out the planned architecutre of the model (i.e. RNN for encoding the input sentence, RNN for encoding the context sentence(s) (when applicable) and attention mechanism to reduce to singular vector followed by feed-forward net for prediction). At the end discuss training, i.e. how negative example sentences are generated from positive ones, and that you will need to compare several hyperparameter configurations.

\begin{enumerate}

\item General Architecutre diagram - sentence and context giving a binary output
\item How different models of context are available - None, 10 relevant wikipedia sentences, 100 triples of knowlege graph, knowledge graph as it is, POS + Named Entity + dependency graph
\item Get into the details of how the sentence and context are going to be modeled, attention mechanism to reduce to a sngular vector followed by feed forward network for prediction
\item Talk about training - how negative sentences are generated from the positive ones and also compare several hyper parameter configurations
\end{enumerate}

We will use  Recurrent neural networks in the master thesis extensively since they are good with sequential data. TensorFlow\footnote{https://www.tensorflow.org/}, a python library for building intensive numerical computation applications and deploy it in multiple platforms such as CPU, GPU etc, will be used to develop RNN architecture. Wikipedia consists of a lot of articles; each article consists of a lot of sentences. Each sentence could be fed as input to the neural network. The different configurations of the neural network will be used and the performance of each will be compared against the other. Some of the configurations of the neural network are

\begin{enumerate}
\item Different layers
\item Learning parameter 
\item Batch size 
\item Activation functions such as Rectified Linear units (ReLu), hyperbolic tangent (tanh) etc; 
\item Epoch number (how long the training should happen)
\item One hot vector vs word embeddings
\item Single/Multiple layer LSTM
\end{enumerate}

The dataset will be split and used for training, validation, and testing of the system. A general rule of thumb is to divide the dataset into 60-20-20 for training, validation, and testing respectively. But if the datasets are in order of millions then it is good enough to have approximately 10000 entries each for validation and testing. The validation dataset is used to test different parameter configurations and select the optimal values.

It is important to do pre-processing on the dataset which involves a series of steps.
\begin{enumerate}
\item removal of common words such as articles for e.g a, the etc; 
\item lemmatization of verbs which involves bringing the conjugated verbs into its root form 
\end{enumerate}

Since we use Wikipedia as a proxy for authentic information, each sentence in the Wikipedia is considered as true news. There is hardly any false news present. For training to be successful to build a good fake news detector, it should be ensured that more or less equal amount of data should be present for both true and fake news. The construction of false news is tricky and we will use different natural language processing (NLP) methods and compare which one is giving good results. Spacy\footnote{https://www.spacy.io}, a python library to do NLP tasks will be used for extracting parts of speech, named entity and dependency relations. Some techniques are 
\begin{enumerate}
\item Swap the words randomly in each and every sentence extracted from Wikipedia
\item Replace the random words picked in a sentence with a random word from the corpus
\item Replace the words in a sentence based on their parts of speech. In this method, a vocabulary of words is maintained for each Parts of Speech section and randomly one of it is chosen
\item Replace the words in each sentence with their opposites extracted from WordNet\footnote{https://wordnet.princeton.edu/}
\item Extract the dependency tree for each sentence and select an appropriate one from other sentences to replace it
\end{enumerate}

% Background Knowledge: Wikidata or BabelNet (knowledge graphs), http://babelfy.org/ for entity resolution

The above methods involve modeling the content of Wikipedia in such a way that the system can understand it better. These procedures are completely automatic in building the training examples and none of the features are manually crafted. The RNN and different configurations of the system along with unique methods in constructing the training data will be able to understand Wikipedia better. This will be an improved semantic understanding of Wikipedia and by using this fake news detection can be carried out in a platform agnostic manner. 

% ----------------------------------------------------------------------------
\section{Evaluation}

The evaluation can be done both internally and externally. The system can be evaluated based on how well it is able to understand Wikipedia. This is done by inputting random sentences or the sentences taken from Wikipedia and check whether it is present or not. 

In addition to that, the system can be evaluated as a fake news detector. There are two ways by which the test dataset can be prepared. 

\begin{enumerate}
\item There are two comprehensive open datasets available. One of it is published in Kaggle\footnote{https://www.kaggle.com/mrisdal/fake-news} which consists of structured data of 13K size. The LIAR dataset is published by Wang \cite{Wang2017} which is mined from politifact.com and it consists approximately 13K entries.
\item Fake news can be obtained directly from facts checking websites such as snopes.com, politifact.com, etc., by crawling or using the API's provided.
\end{enumerate}

The baseline system is a very simple RNN and the improvements are added on top of it. The following are the factors which are used to compare the baseline with the improved system
\begin{enumerate}
\item How good the system is able to classify the non-wikipedia news / fake news?
\item How fast the model is build up?
\end{enumerate}

The goodness of the system can be measured by metrics such as Precision, Recall, F1 measure, and Accuracy. The F1 measure is preferred since it includes both false positive and false negatives. The speed of the system can be measured iteratively by calculating the results of the loss function. In this master thesis, the focus is mainly on improving the accuracy of the system and the speed of the system can be improved by using good hardware.  

\newpage

% ----------------------------------------------------------------------------
\section{Organizational matters}

\begin{tabbing}
Duration of work: \hspace{1.1cm} \= \StartDate{} -- \EndDate{}\\
\vspace{0.5ex}Candidate:	\> \myName{}\\
\vspace{0.5ex}E-Mail:	\> \emailID{}\\
\vspace{0.5ex}Student number: \> \matriculationID{}\\
\vspace{0.5ex}Primary supervisor: \> \expert{}\\
Supervisor: \> \supervisor{}\\
Secondary supervisor: \> \secondSupervisor{}\\
\end{tabbing}

% ----------------------------------------------------------------------------

\section{Time schedule}

\begin{itemize}
	\item Introduction and Literature: 01-May-2018 – 30-June-2018
	\item Initial phase: 01-July-2018 – 15-Sep-2018
	\begin{itemize}
		\item Prototyping: 01-July-2018 – 30-July-2018
		\item ML pipeline implementation: 01-Aug-2018 – 15-Aug-2018
        \item Baseline implementation: 16-Aug-2018 – 30-Aug-2018
        \item Testing and refining: 01-Sep-2018 – 15-Sep-2018
	\end{itemize}
	\item Development phase: 16-Sep-2018 – 30-Dec-2018
	\begin{itemize}
		\item RNN with different configuration: 16-Sep-2018 – 30-Oct-2018
		\item Comprehend benchmark results: 16-Oct-2018 – 21-Oct-2018
		\item Analyse and figure out improvements: 22-Oct-2018 – 30-Oct-2018
        \item Improvisation using NLP techniques: 01-Nov-2018 – 30-Dec-2018
        \item Comprehend benchmark results: 16-Dec-2018 – 30-Dec-2018
	\end{itemize}
	\item Final phase: 01-Jan-2019 – 01-Feb-2019
	\begin{itemize}
	 	\item Comprehend Benchamark results: 01-Jan-2019 – 07-Jan-2019
		\item Revision: 08-Jan-2019 – 22-Jan-2019
		\item Thesis report: 01-Jan-2019 – 30-Jan-2019
	\end{itemize}
\end{itemize}

A meeting with Lukas Schmelzeisen will happen approximately once in two weeks to discuss about the progress made and set the targets and milestones for subsequent weeks. 

% ----------------------------------------------------------------------------
\bibliographystyle{alpha}
\newpage
\bibliography{bib}
\newpage
% ----------------------------------------------------------------------------
\section{Signatures}

\vspace{3cm}
\begin{tabular}{ccc}
  --------------------------------------------------- &  & ---------------------------------------------------\\
  \myName{} &  & \expert{}  \\ \vspace{3cm}
   &  &   \\
  --------------------------------------------------- &  & ---------------------------------------------------\\
  \supervisor{} &  & \secondSupervisor{}  \\ \vspace{3cm}
   &  &   \\
\end{tabular}

\newpage
% ----------------------------------------------------------------------------
\section{Declaration of Authorship}
I hereby declare that the thesis submitted is my own unaided work. All direct or indirect sources used are acknowledged as references.

I am aware that the thesis in digital form can be examined for the use of unauthorized aid and in order to determine whether the thesis as a whole or parts incorporated in it may be deemed as plagiarism. For the comparison of my work with existing sources I agree that it shall be entered in a database where it shall also remain after examination, to enable comparison with future theses submitted. Further rights of reproduction and usage, however, are not granted here.

This paper was not previously presented to another examination board and has not been published.

\vspace{3cm}
\begin{tabular}{ccc}

  Koblenz, on \today &  &  \\
     &  & ---------------------------------------------------\\
   &  & \myName{}  \\
\end{tabular}



\end{document}









