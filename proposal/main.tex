\documentclass[a4paper, 11pt]{article}
\usepackage{a4wide}
\usepackage[ngerman,english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{ifthen}
\usepackage{bibgerm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{graphicx}
\usepackage{blindtext}

\topmargin 0cm \textheight 23cm \parindent0cm

% ---------------------------------------------
%	Commands definition
% ---------------------------------------------

\newcommand{\myName}{Kandhasamy Rajasekaran}
\newcommand{\emailID}{kandhasamy@uni-koblenz.de}
\newcommand{\matriculationID}{216100855}

\newcommand{\Title}{Fake News Detection using Neural Network models of Wikipedia}
\newcommand{\StartDate}{01.07.2018}
\newcommand{\EndDate}{31.12.2018}
\newcommand{\subject}{Institute for Web Science and Technologies}
\newcommand{\expert}{Prof. Dr. Steffen Staab}%inkl. Titel
\newcommand{\supervisor}{Supervisor????} %inkl. Titel
\newcommand{\scndSupervisor}{Lukas Schmelzeisen} %inkl. Titel
\newcommand{\type}{Master Thesis}

\newcommand{\enhanced}{Enhanced Stitched-Viewport Screenshot}
\newcommand{\stitched}{Stitched-Viewport Screenshot}
\newcommand{\expanded}{Expanded-Viewport Screenshot}


\begin{document}
% ---------------------------------------------
%	Title
% ---------------------------------------------
\selectlanguage{ngerman}
Universit\"{a}t Koblenz - Landau \hfill \today

Department f\"{u}r Informatik,

\subject{}

\expert{}

\supervisor{}

\scndSupervisor{}

\begin{center}
	\large{\bf \type{}  \myName{}}

	\vspace*{0.5cm}

	\large{\bf \Title}
\end{center}

\setlength{\parskip}{1.5ex plus0.5ex minus 0.5ex}
% -----------------------------------------------------------------------------
%	Content
% -----------------------------------------------------------------------------
\selectlanguage{english}
\begin{abstract}
\frenchspacing
\noindent
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi nulla ligula, blandit quis mi ut, pellentesque mattis nisi. Praesent elementum diam sed nibh lobortis dictum. Sed eleifend posuere magna, ac rhoncus nisi feugiat in. Donec accumsan ac leo in cursus. Vestibulum id elit lacus. Ut nec rhoncus dolor, quis laoreet dui. Donec luctus fringilla elit. Sed suscipit dolor dolor, sed consequat neque ornare non. Nunc laoreet, metus ac congue efficitur, neque felis varius metus, in pretium dui nibh eget nisi. Integer maximus porta turpis, et dignissim dolor tempus ac. In hac habitasse platea dictumst. Vivamus pretium dui massa, in volutpat orci semper in. Aliquam lorem elit, gravida pretium aliquet id, laoreet eget sem. Mauris mauris lacus, vehicula at felis sit amet, rutrum efficitur est. Pellentesque auctor vestibulum risus sit amet consectetur. Nunc lobortis velit sed nunc aliquam, non egestas nisl vulputate. Nam posuere mauris lectus, nec ullamcorper ipsum placerat eget. Duis quis justo sit amet nunc facilisis ullamcorper.
\end{abstract}
% -----------------------------------------------------------------------------
\section{Introduction}
\frenchspacing

%News/Information - how important it is to the society, how society advances
%Fake news - Fake news in web -  its prevalence and the disadvantages, consequences of it
%Compare against reliable source of information - that is the way to detect it, experts from each domain give their views and a consolidated decision

%Manual correction is not possible. Web has seen an unprecedented growth. It is important to have automated means.
%Wikipedia - say 1 or 2 sentences about it being a source of information - different subject matters - wide coverage. Frequency of updates
%About the reliability of source of information in wikipedia

%Wikipedia is peer reviewed as against the newspaper

%Fake news detection using wikipedia as a ground reality
%What class of fake news it can solve ?? - may be at the beginning specify different classes of fake news

Humans evolved and became a superior race than other species only by being able to pass the information to its descendants. The society advances as a whole by improving its collective intelligence and that happens only by sharing. With the advent of technology systems such as web, it is possible for anyone to become a publisher and publish their thoughts, ideas to the entire world. As much as it became easier to disseminate information it also brought in a lot of negative effects. One of it is the spread of false news or fake news. The dissemination of false news can harm the society very bad and it can reduce the progress or worsen the state of the society. There are several examples such as a misleading news refering to a fire in a US school made a lot of chaos in traffic and accidents because parents were rushing to save their kids. Humans have been relying on their collective intelligence and advice from experts on a particular domain to handle the fake news. There are several organizations such as snopes, factcheck.org, politfact which tags many information with a fact meter to label the authenticity of information. Although this is fairly good, it is not scalable and unmatchable to the rate at which the information is created. With many social networks, blogging sites we have seen a unparallel rise in the volume of content being generated and a manual intervention to cross check their authenticity is clearly no match. But overall the main idea to handle false news is to check against reliable source of information and claim its integrity.

Wikipedia is a crowdsourced online encylopedia which has versatile topics and a whole range of articles for each topic. It has been listed as one of the famous 10 websites or the one of the 10 most used website. The range of subject covered is very wide and it comes in different languages. The frequency of the udpate is really high and every content is peer reviewed by other as against the process in newspaper. Although the checking need not be done by experts opinion the reliability of common public on wikipeida is highly visible. 

The focus of the master thesis is to use wikipedia a ground reality or as source of experts opinion and use this knowledge to cross check the claims which in the form of tweets, blogs, sentences through automated means. Recurrent Neural Network are a special kind of neural network models for sequence data. They are used to understand wikipedia and acts a bot to check the news as fake or not. In this we assume whatever information present in wikipedia is truthy and we rely on the collective and collaborative effort of humans to build this enormous information resource.
% ----------------------------------------------------------------------------
\section{Related work}

There are many attempts being made to counter attack fake news. Most of them are imbibed into one particular platform and uses the news, user characteristics, who shared it and what is their credibility and how the diffusion have happened. They used many supervised machine learning algorithms which uses handcrafted features and obtain substantial results. 

Giovanni Luca Ciampaglia et al. used DBPedia for checking computationally whether a given information is factual or not. The work involves using the knowledge graph built from DBPedia which represents infobox section in Wikipedia. This represents only non-controversial and factual information which is analagous to human collected information. The methodology formulates the problem of checking facts into a network analysis problem which is finding the shortest path between nodes (subject and object of a sentence) in a graph. The aggregated generalities of nodes along a path in a weighted undirected graph is used as a metric for measuring the authenticity of information. The more the elements are generic the weaker the truthfulness is.  The genericness of a node is obtained by the degree of that node - no. of nodes connected to that node. The truthfulness of the information is improved if there exists at least one path from subject to an object with minimal non-generic nodes. This approach exploits the indirect connections to a great extent with distance constraints in a knowledge graph. The approach gave promising results when tested with datasets containing simple factual information about history, geography, entertainment and biography. 

limitations
-----------
1)primitive - the complexity of news dealt with is very primitive. Current fake news are very complex and subtle when it comes to the ambiquities
2) Semantic proximity methodology is simple and primitive


But it is a good initial step towards an automated fact checker system.


What improvements are we going to bring?
----------------------------------------
1) Ours would also be primitive :-) May be slightly better :-)
2) Frequency of conversion of wikipedia content into DBPedia is lesser ??
3) DBPedia does not cover the whole of wikipedia ???? 

Things to do:
-------------
1) Find the answers for above questions
2) Get the dataset that we can use?
3) Try to look for a demo of this system

Jing Ma et al. efforts were focused on building a recurrent neural network (RNN) to detect rumors from Microblogs effectively. The social context information of a post and all its relevant posts such as comments or retweets is modeled as variable-length time series.  RNNs with different configurations such as using one or two layers of GRU and LSTM are very good in capturing long distance dependencies of temporal and textual representations of posts under supervision. This method completely avoids all the handcrafted feature engineering efforts which are biased and time consuming. It produces better results with datasets from Twitter and Sina Weibo than all of the traditional Machine Learning methods. RNNs with two layers of GRU gave the best results and it was also very quick in predicting the rumor than the average time from debunking services.  

%Attempts made to figure out fake news in the past
%Manually curated facts dataset
%If there are any automatic ones
%It would be nice to see a forum/wiki (I hope not wikipedia :-)) being used and the methodology that have been used

%Research by Filippo Menczer - uses the DBPedia which is a structured wikipedia and use it to cross check claims
%https://www.mendeley.com/viewer/?fileId=631c34ce-5e90-722b-2974-6d71a44ad9ef&documentId=4c0051e0-64fb-3e10-959e-ac8046c9002b

%Twitter has been used - 
%https://885d47c6-a-62cb3a1a-s-sites.googlegroups.com/site/iswgao/home/ijcai16.pdf?attachauth=ANoY7cq8X9N6sGSW7wLfGloRItBkaFO1a2ELhv4s2rWN8VXGMbTwuTjUh_uGRA5vslyvOT1UDNx5wpxCWdZLNeaBcqvLO9N3dfgJfhphfDNv3pZh1P69EgHWJZeg2wGjSGDI-bhBo4VHDDwFqqM-JDoNCigNHEoTK3zDi4Dn6mGIAMcmOUfs6KrEBdAk0QUpJWPmrARCylfQe41FkVkZ0Hkpo2w-akFauQ%3D%3D&attredirects=0

%Social networks - using usage patterns in social networks- 
%https://www.mendeley.com/viewer/?fileId=6e662e87-8846-c9c3-5a3b-505f835bd89b&documentId=f238c7a0-21a1-3cab-9ed4-1c864aa0011c

% ----------------------------------------------------------------------------
\section{Approach}

Deep learning - using RNN network architecture for sequential models
One hot representation or word vector
%https://www.mendeley.com/viewer/?fileId=3eea8d80-58e9-51f5-5b45-945420ba7577&documentId=a45b2cdb-dae1-3d54-8d47-395580293df7

Usage of LSTM - to support long range dependencies
Wordvectors? 
Language modeling? - Based on the probability of occurrence - we can predict how likely the sentence can occur or predict the next word. Given a news - predict how likely it can happen. But this may  not give a good result - so, can be used as a baseline

Some research questions are

What would be the best word vector representation for this problem - how do we prepare it? - have to generalize it
How do we create negative examples? What is the best way to do it for this kind of problem?
What lies can be detected effectively? This will explain what level of information is updated in wikipedia in each domain - constructing the dataset would be tricky. Can look at ways how to construct datasets based on the domain


% ----------------------------------------------------------------------------
\section{Evaluation}

The training, evaluation and test dataset needs to be curated
Enough data should be present for each bin such as facts, slightly distorted lie, blatant lies 

Extract sentence from wikipedia and give it as it is
Distort the sentence by swapping and give it 

Construct good facts and lies outside - a proper labelled dataset and see how it works
\newpage

% ----------------------------------------------------------------------------
\section{Organizational matters}

\begin{tabbing}
Duration of work: \hspace{1.1cm} \= \StartDate{} -- \EndDate{}\\
\vspace{0.5ex}Candidate:	\> \myName{}\\
\vspace{0.5ex}E-Mail:	\> \emailID{}\\
\vspace{0.5ex}Student number: \> \matriculationID{}\\
\vspace{0.5ex}Primary supervisor: \> \expert{}\\
Supervisor: \> \supervisor{}\\
Secondary supervisor: \> \scndSupervisor{}\\
\end{tabbing}

% ----------------------------------------------------------------------------

\section{Time schedule}

This needs to be changed. But for now just a filler.

\begin{itemize}
	\item Introduction and Literature: 03.05.17 – 15.06.17
	\item Methodology: 09.06.17 – 30.07.17
	\begin{itemize}
		\item Approach concept: 09.06.17 – 22.06.17
		\item Implementation Plan: 18.06.17 – 29.06.17
        \item Implementation: 29.06.17 – 30.07.17
        \item Testing and refining: 01.07.17 – 30.07.17
	\end{itemize}
	\item Approach results: 31.07.17 – 20.08.17
	\begin{itemize}
		\item Sampling: 31.07.17 – 12.08.17
		\item Interpretation: 13.08.17 – 20.08.17
	\end{itemize}
	\item Evaluation: 21.08.17 – 25.09.17
	\begin{itemize}
		\item Preparing evaluation: 21.08.17 – 01.09.17
		\item Conducting evaluation: 02.09.17 – 11.09.17
		\item Analyzing results: 12.09.17 – 25.09.17
	\end{itemize}
	\item Revision: 26.09.17 – 31.10.17
\end{itemize}


% ----------------------------------------------------------------------------
\bibliographystyle{alpha}
\bibliography{bib}
\newpage
% ----------------------------------------------------------------------------
\section{Signatures}

\vspace{3cm}
\begin{tabular}{ccc}
  --------------------------------------------------- &  & ---------------------------------------------------\\
  \myName{} &  & \expert{}  \\ \vspace{3cm}
   &  &   \\
  --------------------------------------------------- &  & ---------------------------------------------------\\
  \supervisor{} &  & \scndSupervisor{}  \\ \vspace{3cm}
   &  &   \\
\end{tabular}

\newpage
% ----------------------------------------------------------------------------
\section{Declaration of Authorship}
I hereby declare that the thesis submitted is my own unaided work. All direct or indirect sources used are acknowledged as references.

I am aware that the thesis in digital form can be examined for the use of unauthorized aid and in order to determine whether the thesis as a whole or parts incorporated in it may be deemed as plagiarism. For the comparison of my work with existing sources I agree that it shall be entered in a database where it shall also remain after examination, to enable comparison with future theses submitted. Further rights of reproduction and usage, however, are not granted here.

This paper was not previously presented to another examination board and has not been published.

\vspace{3cm}
\begin{tabular}{ccc}

  Koblenz, on \today &  &  \\
     &  & ---------------------------------------------------\\
   &  & \myName{}  \\
\end{tabular}



\end{document}









