@article {Vosoughi1146,
	author = {Vosoughi, Soroush and Roy, Deb and Aral, Sinan},
	title = {The spread of true and false news online},
	volume = {359},
	number = {6380},
	pages = {1146--1151},
	year = {2018},
	doi = {10.1126/science.aap9559},
	publisher = {American Association for the Advancement of Science},
	abstract = {There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \~{}3 million people. False news reached more people than the truth; the top 1\% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed.Science, this issue p. 1146We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98\% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/359/6380/1146},
	eprint = {http://science.sciencemag.org/content/359/6380/1146.full.pdf},
	journal = {Science}
}

@article {Lazer1094,
	author = {Lazer, David M. J. and Baum, Matthew A. and Benkler, Yochai and Berinsky, Adam J. and Greenhill, Kelly M. and Menczer, Filippo and Metzger, Miriam J. and Nyhan, Brendan and Pennycook, Gordon and Rothschild, David and Schudson, Michael and Sloman, Steven A. and Sunstein, Cass R. and Thorson, Emily A. and Watts, Duncan J. and Zittrain, Jonathan L.},
	title = {The science of fake news},
	volume = {359},
	number = {6380},
	pages = {1094--1096},
	year = {2018},
	doi = {10.1126/science.aao2998},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/359/6380/1094},
	eprint = {http://science.sciencemag.org/content/359/6380/1094.full.pdf},
	journal = {Science}
}


@article{Ishikawa2013,
author = {Ishikawa, Yoshiharu and Li, Jianzhong and Wang, Wei and Zhang, Rui and Zhang, Wenjie},
file = {::},
title = {{Web Technologies and Applications}},
year = {2013}
}
@inproceedings{Lee2010,
abstract = {Web-based social systems enable new community-based opportu- nities for participants to engage, share, and interact. This com- munity value and related services like search and advertising are threatened by spammers, content polluters, and malware dissemi- nators. In an effort to preserve community value and ensure long- term success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deploy- ment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deploy- ment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives. Categories},
author = {Lee, Kyumin and Caverlee, James and Webb, Steve},
booktitle = {Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '10},
doi = {10.1145/1835449.1835522},
file = {::},
isbn = {9781450301534},
title = {{Uncovering social spammers}},
year = {2010}
}
@inproceedings{Hu2013,
abstract = {Abstract The availability of microblogging , like Twitter and Sina Weibo, makes it a popular platform for spammers to unfairly overpower normal users with unwanted content via social networks, known as social spamming . The rise of social spamming can significantly hinder ...},
author = {Hu, Xia and Tang, Jiliang and Zhang, Yanchao and Liu, Huan},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {::},
isbn = {9781577356332},
issn = {10450823},
title = {{Social spammer detection in microblogging}},
year = {2013}
}
@inproceedings{Kwon2013,
abstract = {â€”The problem of identifying rumors is of practical importance especially in online social networks, since infor- mation can diffuse more rapidly and widely than the offline counterpart. In this paper, we identify characteristics of rumors by examining the following three aspects of diffusion: temporal, structural, and linguistic. For the temporal characteristics, we propose a new periodic time series model that considers daily and external shock cycles, where the model demonstrates that rumor likely have fluctuations over time. We also identify key structural and linguistic differences in the spread of rumors and non-rumors. Our selected features classify rumors with high precision and recall in the range of 87{\%} to 92{\%}, that is higher},
author = {Kwon, Sejeong and Cha, Meeyoung and Jung, Kyomin and Chen, Wei and Wang, Yajun},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2013.61},
file = {::},
isbn = {978-0-7695-5108-1},
issn = {15504786},
keywords = {Diffusion Network,Rumor,Sentiment Analysis,Social Media,Time Series},
title = {{Prominent features of rumor propagation in online social media}},
year = {2013}
}
@inproceedings{Wu2015,
abstract = {This paper studies the problem of automatic detection of false rumors on Sina Weibo, the popular Chinese microblogging social network. Traditional feature-based approaches extract features from the false rumor message, its author, as well as the statistics of its responses to form a flat feature vector. This ignores the propagation structure of the messages and has not achieved very good results. We propose a graph-kernel based hybrid SVM classifier which captures the high-order propagation patterns in addition to semantic features such as topics and sentiments. The new model achieves a classification accuracy of 91.3{\%} on randomly selected Weibo dataset, significantly higher than state-of-the-art approaches. Moreover, our approach can be applied at the early stage of rumor propagation and is 88{\%} confident in detecting an average false rumor just 24 hours after the initial broadcast.},
author = {Wu, Ke and Yang, Song and Zhu, Kenny Q.},
booktitle = {Proceedings - International Conference on Data Engineering},
doi = {10.1109/ICDE.2015.7113322},
file = {::},
isbn = {9781479979639},
issn = {10844627},
title = {{False rumors detection on Sina Weibo by propagation structures}},
year = {2015}
}
@inproceedings{Yang2012,
abstract = {The problem of gauging information credibility on social networks has received considerable attention in recent years. Most previous work has chosen Twitter, the world's largest micro-blogging platform, as the premise of research. In this work, we shift the premise and study the problem of information credibility on Sina Weibo, China's leading micro-blogging service provider. With eight times more users than Twitter, Sina Weibo is more of a Facebook-Twitter hybrid than a pure Twitter clone, and exhibits several important characteristics that distinguish it from Twitter. We collect an extensive set of microblogs which have been confirmed to be false rumors based on information from the official rumor-busting service provided by Sina Weibo. Unlike previous studies on Twitter where the labeling of rumors is done manually by the participants of the experiments, the official nature of this service ensures the high quality of the dataset. We then examine an extensive set of features that can be extracted from the microblogs, and train a classifier to automatically detect the rumors from a mixed set of true information and false information. The experiments show that some of the new features we propose are indeed effective in the classification, and even the features considered in previous studies have different implications with Sina Weibo than with Twitter. To the best of our knowledge, this is the first study on rumor analysis and detection on Sina Weibo.},
archivePrefix = {arXiv},
arxivId = {10.1145/2350190.2350203},
author = {Yang, Fan and Liu, Yang and Yu, Xiaohui and Yang, Min},
booktitle = {Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics - MDS '12},
doi = {10.1145/2350190.2350203},
eprint = {2350190.2350203},
file = {::},
isbn = {9781450315463},
primaryClass = {10.1145},
title = {{Automatic detection of rumor on Sina Weibo}},
year = {2012}
}
@inproceedings{Liu2015,
abstract = {In this paper, we propose the first real time rumor debunk- ing algorithm for Twitter. We use cues from â€˜wisdom of the crowds', that is, the aggregate â€˜common sense' and in- vestigative journalism of Twitter users. We concentrate on identification of a rumor as an event that may comprise of one or more conflicting microblogs. We continue monitoring the rumor event and generate real time updates dynamically based on any additional information received. We show us- ing real streaming data that it is possible, using our ap- proach, to debunk rumors accurately and efficiently, often much},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00306v1},
author = {Liu, Xiaomo and Nourbakhsh, Armineh and Li, Quanzhi and Fang, Rui and Shah, Sameena},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management - CIKM '15},
doi = {10.1145/2806416.2806651},
eprint = {arXiv:1601.00306v1},
file = {::},
isbn = {9781450337946},
issn = {15504786},
title = {{Real-time Rumor Debunking on Twitter}},
year = {2015}
}
@inproceedings{Ma2015,
abstract = {ABSTRACT Automatically identifying rumors from online social media especially microblogging websites is an important research issue. Most of existing work for rumor detection focuses on modeling features related to microblog contents, users and propagation patterns, but ignore the importance of the variation of these social context features during the message propagation over time. In this study, we propose a novel approach to capture the temporal characteristics of these features based on the time series of rumoréˆ¥æªš lifecycle, for which time series modeling technique is applied to incorporate various social context information. Our experiments using the events in two microblog datasets confirm that the method outperforms state-of-the-art rumor detection approaches by large margins. Moreover, our model demonstrates strong performance on detecting rumors at early stage after their initial broadcast.},
author = {Ma, Jing and Gao, Wei and Wei, Zhongyu and Lu, Yueming and Wong, Kam-Fai},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management - CIKM '15},
doi = {10.1145/2806416.2806607},
file = {::},
isbn = {9781450337946},
title = {{Detect Rumors Using Time Series of Social Context Information on Microblogging Websites}},
year = {2015}
}
@article{Flanagin,
abstract = {People increasingly rely on Internet and web-based information despite evidence that it is potentially inaccurate and biased. Therefore, this study sought to assess people's perceptions of the credibility of various categories of Internet information compared to similar information provided by other media. The 1,041 respondents also were asked about whether they verified Internet information. Overall, respondents re-ported they considered Internet information to be as credible as that obtained from television, radio, and magazines, but not as credible as newspaper information. Credibility among the types of information sought, such as news and entertainment, varied across media channels. Respondents said they rarely verified web-based information, although this too varied by the type of information sought. Levels of experienceand how respondents perceived the credibility of information were related to whether they verified information. This study explores the social relevance of the findings and discusses them in terms of theoretical knowledge of advanced communication technologies. The Internet has quickly become a viable technology used by an estimated 130 million people' in 171 countries2 for a variety of communica-tion and information-sharing tasks. Internet technologies have been applied to education; have stimulated electronic commerce,4 have been used to develop online communities and cultures,5 and have helped organizations develop communication via intranets6 However, although information obtained via the Internet is abundant, easily available, and often comprehensive, it can differ from information obtained via other media sources in several respects. For instance, web-based information typically undergoes an editorial process prior to "publication" that may differ greatly from that of other media content. In addition, people are still experimenting with strategies to make sense of web-based informa-tion. Among the potential results of this relatively unchecked information flow and individuals' nascent sensemaking strategies is the possibility that information is intentionally or unintentionally inaccurate, biased, or mis-leading. Indeed, the growth of the Internet has seen an attendant growth of online fraud and mi{\~{}}information.{\~{}} Misinformation, of course, is not new with the Internet. However, many of the existing institutional, structural, and cognitive methods people employ to discern the relative value or accuracy of information (e.g., estab-Andrew I. Flanagin and Miriam I. Metzger are both assistant professors in the},
author = {Flanagin, Andrew 1 and Metzger, Miriam J},
file = {:home/kandy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Flanagin, Metzger - Unknown - PERCEPTIONS OF INTERNET INFOMTION CREDIBILITY.pdf:pdf},
title = {{PERCEPTIONS OF INTERNET INFOMTION CREDIBILITY}}
}
@inproceedings{Castillo2011,
abstract = {We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally. On this paper we focus on automatic methods for assessing the credibility of a given set of tweets. Specifically, we analyze microblog postings related to "trending" topics, and classify them as credible or not credible, based on features extracted from them. We use features from users' posting and re-posting ("re-tweeting") behavior, from the text of the posts, and from citations to external sources. We evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of Twitter postings. Our results shows that there are measurable differences in the way messages propagate, that can be used to classify them automatically as credible or not credible, with precision and recall in the range of 70{\%} to 80{\%}.},
author = {Castillo, Carlos and Mendoza, Marcelo and Poblete, Barbara},
booktitle = {Proceedings of the 20th international conference on World wide web - WWW '11},
doi = {10.1145/1963405.1963500},
file = {:home/kandy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castillo, Mendoza, Poblete - 2011 - Information credibility on twitter.pdf:pdf},
isbn = {9781450306324},
title = {{Information credibility on twitter}},
year = {2011}
}
@misc{Wales2005,
abstract = {The journal Nature preformed a comparative analysis of scientific data entry accuracy between Wikipedia, a free online encyclopedia that anyone can edit, and Encyclopedia Britannica. The average science entry in Wikipedia contains around 4 inaccuracies; Britannica, about 3.},
author = {Wales, Jimmy},
booktitle = {Nature},
doi = {10.1038/438900a},
file = {::},
isbn = {0028-0836},
issn = {14764687},
pmid = {16355180},
title = {{Internet encyclopaedias go head to head}},
year = {2005}
}
@article{DeDeo2013,
abstract = {We investigate the computational structure of a paradigmatic example of distributed social interaction: that of the open-source Wikipedia community. We examine the statistical properties of its cooperative behavior, and perform model selection to determine whether this aspect of the system can be described by a finite-state process, or whether reference to an effectively unbounded resource allows for a more parsimonious description. We find strong evidence, in a majority of the most-edited pages, in favor of a collective-state model, where the probability of a "revert" action declines as the square root of the number of non-revert actions seen since the last revert. We provide evidence that the emergence of this social counter is driven by collective interaction effects, rather than properties of individual users.},
archivePrefix = {arXiv},
arxivId = {1212.0018},
author = {DeDeo, Simon},
doi = {10.1371/journal.pone.0075818},
eprint = {1212.0018},
file = {::},
isbn = {0027-8424},
issn = {19326203},
journal = {PLoS ONE},
pmid = {24130745},
title = {{Collective Phenomena and Non-Finite State Computation in a Human Social System}},
year = {2013}
}
@inproceedings{Priedhorsky2007,
abstract = {Wikipedia's brilliance and curse is that any user can edit any of the encyclopedia entries. We introduce the notion of the impact of an edit, measured by the number of times the edited version is viewed. Using several datasets, including recent logs of all article views, we show that an overwhelm- ing majority of the viewed words were written by frequent editors and that this majority is increasing. Similarly, using the same impact measure, we show that the probability of a typical article view being damaged is small but increasing, and we present empirically grounded classes of damage. Fi- nally, we make policy recommendations for Wikipedia and other wikis in light of these findings},
author = {Priedhorsky, Reid and Chen, Jilin and Lam, Shyong (Tony) K. and Panciera, Katherine and Terveen, Loren and Riedl, John},
booktitle = {Proceedings of the 2007 international ACM conference on Conference on supporting group work  - GROUP '07},
doi = {10.1145/1316624.1316663},
file = {::},
isbn = {9781595938459},
issn = {1595938451},
title = {{Creating, destroying, and restoring value in wikipedia}},
year = {2007}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
file = {::},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Lazer2017,
annote = {Psychology of fake news},
author = {Lazer, David and Baum, Matthew and Grinberg, Nir and Friedland, Lisa and Joseph, Kenneth and Hobbs, Will and Mattsson, Carolina and {Benkler Harvard}, Yochai and Watts, Duncan},
file = {::},
title = {{Combating Fake News: An Agenda for Research and Action Drawn from presentations by}},
year = {2017}
}
@article{Hochreiter1997,
abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. },
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Goldberg2016,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
author = {Goldberg, Yoav},
file = {::},
journal = {Journal of Artificial Intelligence Research},
pages = {345--420},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
volume = {57},
year = {2016}
}
@article{Goldsborough2016,
abstract = {Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released {\$}\backslashtextit{\{}TensorFlow{\}}{\$}, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.},
archivePrefix = {arXiv},
arxivId = {1610.01178},
author = {Goldsborough, Peter},
doi = {10.1017/CBO9781107415324.004},
eprint = {1610.01178},
file = {::},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{A Tour of TensorFlow}},
year = {2016}
}
@article{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translat-ing natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connec-tionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have tradition-ally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and paral-lel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
author = {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
file = {::},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
year = {2015}
}
@article{Ciampaglia2015,
abstract = {Traditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this approach is feasible with efficient computational techniques. We evaluate this approach by examining tens of thousands of claims related to history, entertainment, geography, and biographical information using a public knowledge graph extracted from Wikipedia. Statements independently known to be true consistently receive higher support via our method than do false ones. These findings represent a significant step toward scalable computational fact-checking methods that may one day mitigate the spread of harmful misinformation.},
archivePrefix = {arXiv},
arxivId = {1501.03471v1},
author = {Ciampaglia, Giovanni Luca and Shiralkar, Prashant and Rocha, Luis M. and Bollen, Johan and Menczer, Filippo and Flammini, Alessandro},
doi = {10.1371/journal.pone.0128193},
eprint = {1501.03471v1},
file = {::},
isbn = {19326203 (Electronic)},
issn = {19326203},
journal = {PLoS ONE},
pmid = {26083336},
title = {{Computational fact checking from knowledge networks}},
year = {2015}
}
@article{Ma,
abstract = {Microblogging platforms are an ideal place for spreading rumors and automatically debunking ru-mors is a crucial problem. To detect rumors, ex-isting approaches have relied on hand-crafted fea-tures for employing machine learning algorithms that require daunting manual effort. Upon facing a dubious claim, people dispute its truthfulness by posting various cues over time, which generates long-distance dependencies of evidence. This pa-per presents a novel method that learns continuous representations of microblog events for identifying rumors. The proposed model is based on recur-rent neural networks (RNN) for learning the hidden representations that capture the variation of contex-tual information of relevant posts over time. Ex-perimental results on datasets from two real-world microblog platforms demonstrate that (1) the RNN method outperforms state-of-the-art rumor detec-tion models that use hand-crafted features; (2) per-formance of the RNN-based algorithm is further improved via sophisticated recurrent units and ex-tra hidden layers; (3) RNN-based method detects rumors more quickly and accurately than existing techniques, including the leading online rumor de-bunking services.},
author = {Ma, Jing and Gao, Wei and Mitra, Prasenjit and Kwon, Sejeong and Jansen, Bernard J and Wong, Kam-Fai and Cha, Meeyoung},
file = {::},
title = {{Detecting Rumors from Microblogs with Recurrent Neural Networks}}
}
@inproceedings{Tacchini2017,
abstract = {In recent years, the reliability of information on the Internet has emerged as a crucial issue of modern society. Social network sites (SNSs) have revolutionized the way in which information is spread by allowing users to freely share content. As a consequence, SNSs are also increasingly used as vectors for the diffusion of misinformation and hoaxes. The amount of disseminated information and the rapidity of its diffusion make it practically impossible to assess reliability in a timely manner, highlighting the need for automatic hoax detection systems. As a contribution towards this objective, we show that Facebook posts can be classified with high accuracy as hoaxes or non-hoaxes on the basis of the users who "liked" them. We present two classification techniques, one based on logistic regression, the other on a novel adaptation of boolean crowdsourcing algorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users, we obtain classification accuracies exceeding 99{\%} even when the training set contains less than 1{\%} of the posts. We further show that our techniques are robust: they work even when we restrict our attention to the users who like both hoax and non-hoax posts. These results suggest that mapping the diffusion pattern of information can be a useful component of automatic hoax detection systems.},
archivePrefix = {arXiv},
arxivId = {1704.07506},
author = {Tacchini, Eugenio and Ballarin, Gabriele and {Della Vedova}, Marco L. and Moret, Stefano and de Alfaro, Luca},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1257/jep.31.2.211},
eprint = {1704.07506},
file = {::},
issn = {16130073},
title = {{Some like it Hoax: Automated fake news detection in social networks}},
year = {2017}
}
@inproceedings{Wu2018,
abstract = {When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classiication of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Speciically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classiication accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.},
author = {Wu, Liang and Liu, Huan},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining  - WSDM '18},
doi = {10.1145/3159652.3159677},
isbn = {9781450355810},
title = {{Tracing Fake-News Footprints}},
year = {2018}
}

@book {Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={www.deeplearningbook.org},
    year={2016}
}


