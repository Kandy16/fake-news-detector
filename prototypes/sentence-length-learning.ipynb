{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1523273466121,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "hwq4V3xwMbYk",
    "outputId": "12e9efc9-76dd-457a-de65-21b973fa8944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabuloryFile = open('simple-engish-wiki-10-article-vocabulory.txt','r')\n",
    "fileContent = vocabuloryFile.read()\n",
    "vocabulory_read = fileContent.split('\\n')\n",
    "#print(vocabulory_read[0])  \n",
    "vocabuloryFile.close()\n",
    "\n",
    "vocab_to_id_dict = dict(zip(vocabulory_read, range(len(vocabulory_read))))\n",
    "#print(vocab_to_id_dict)\n",
    "id_to_vocab_dict = dict(zip(range(len(vocabulory_read)), vocabulory_read))\n",
    "#print(id_to_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1671, 3671, 3109, 687, 1413, 3272, 3637, 1680, 2158, 4236])\n",
      " list([1653, 342, 1875, 498, 1709, 3842, 2213, 2003, 3986, 2234])\n",
      " list([1670, 3646, 3898, 481, 3304, 3433, 2412, 1661, 314, 517]) ...\n",
      " list([2328, 1469, 1652, 786, 4370]) list([4236, 1701, 1472, 3341, 180])\n",
      " list([3090, 2370, 2581, 975, 45])]\n",
      "[1 1 1 ... 0 0 0]\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (20000, 10)\n",
      "x_test shape: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "train_labels = []\n",
    "for row in np.random.choice(vocabulory_read,size=(10000,10)):\n",
    "    train_features.append([vocab_to_id_dict[word] for word in row])\n",
    "    train_labels.append(1)\n",
    "\n",
    "for row in np.random.choice(vocabulory_read,size=(10000,5)):\n",
    "    train_features.append([vocab_to_id_dict[word] for word in row])\n",
    "    train_labels.append(0)\n",
    "    \n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for row in np.random.choice(vocabulory_read,size=(1000,10)):\n",
    "    test_features.append([vocab_to_id_dict[word] for word in row])\n",
    "    test_labels.append(1)\n",
    "\n",
    "for row in np.random.choice(vocabulory_read,size=(1000,5)):\n",
    "    test_features.append([vocab_to_id_dict[word] for word in row])\n",
    "    test_labels.append(0)\n",
    "    \n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(test_features)\n",
    "print(test_labels)\n",
    "\n",
    "vocab_size = len(vocabulory_read)\n",
    "sentence_size = 10\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# we assign the first indices in the vocabulary to special tokens that we use\n",
    "#Â for padding, as start token, and for indicating unknown words\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(train_features, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "y_train = train_labels\n",
    "x_test = sequence.pad_sequences(test_features, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "\n",
    "y_test = test_labels\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VPLRuDRnvq92"
   },
   "outputs": [],
   "source": [
    "x_len_train = np.array([min(len(x), sentence_size) for x in train_features])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in test_features])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_features))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NMHErNXuBFHr"
   },
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    # Save a reference to the classifier to run predictions later\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=2500)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()\n",
    "#     # Un-comment code to download experiment data from Colaboratory\n",
    "#     from google.colab import files\n",
    "#     model_name = os.path.basename(os.path.normpath(classifier.model_dir))\n",
    "#     ! zip -r {model_name + '.zip'} {classifier.model_dir}\n",
    "#     files.download(model_name + '.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 215179,
     "status": "ok",
     "timestamp": 1520289526441,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "QSNT9KXtt_Up",
    "outputId": "dfbaee5e-3b3d-4c2a-db64-9b3350acdf65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp2k4pfc94/lstm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9a515e4e48>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp2k4pfc94/lstm/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.71657, step = 1\n",
      "INFO:tensorflow:global_step/sec: 18.6358\n",
      "INFO:tensorflow:loss = 0.7091995, step = 101 (5.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.2314\n",
      "INFO:tensorflow:loss = 0.09826494, step = 201 (5.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.3255\n",
      "INFO:tensorflow:loss = 0.05174199, step = 301 (4.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.577\n",
      "INFO:tensorflow:loss = 0.03276434, step = 401 (5.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.589\n",
      "INFO:tensorflow:loss = 0.025187485, step = 501 (4.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.2676\n",
      "INFO:tensorflow:loss = 0.017078552, step = 601 (5.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.7252\n",
      "INFO:tensorflow:loss = 0.014092714, step = 701 (4.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.7476\n",
      "INFO:tensorflow:loss = 0.012309875, step = 801 (5.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.4591\n",
      "INFO:tensorflow:loss = 0.009391485, step = 901 (5.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.2314\n",
      "INFO:tensorflow:loss = 0.007908624, step = 1001 (5.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.7127\n",
      "INFO:tensorflow:loss = 0.006933879, step = 1101 (5.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.9563\n",
      "INFO:tensorflow:loss = 0.0056273444, step = 1201 (5.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.3369\n",
      "INFO:tensorflow:loss = 0.0052079153, step = 1301 (4.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.5655\n",
      "INFO:tensorflow:loss = 0.00444897, step = 1401 (5.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.285\n",
      "INFO:tensorflow:loss = 0.004017634, step = 1501 (5.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.7767\n",
      "INFO:tensorflow:loss = 0.0035083976, step = 1601 (5.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.7421\n",
      "INFO:tensorflow:loss = 0.0032025764, step = 1701 (4.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.3115\n",
      "INFO:tensorflow:loss = 0.0027571055, step = 1801 (5.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.4901\n",
      "INFO:tensorflow:loss = 0.0024679983, step = 1901 (4.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.1467\n",
      "INFO:tensorflow:loss = 0.002207665, step = 2001 (5.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.5564\n",
      "INFO:tensorflow:loss = 0.0021390147, step = 2101 (5.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.4389\n",
      "INFO:tensorflow:loss = 0.0019768623, step = 2201 (6.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.7715\n",
      "INFO:tensorflow:loss = 0.0017984897, step = 2301 (4.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.9271\n",
      "INFO:tensorflow:loss = 0.0016045859, step = 2401 (5.283 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /tmp/tmp2k4pfc94/lstm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.001558994.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-12-18:19:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp2k4pfc94/lstm/model.ckpt-2500\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-12-18:19:11\n",
      "INFO:tensorflow:Saving dict for global step 2500: accuracy = 1.0, accuracy_baseline = 0.5, auc = 1.0, auc_precision_recall = 1.0, average_loss = 1.48657255e-05, global_step = 2500, label/mean = 0.5, loss = 0.0014865726, prediction/mean = 0.5000008\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp2k4pfc94/lstm/model.ckpt-2500\n"
     ]
    }
   ],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def lstm_model_fn(features, labels, mode):    \n",
    "    # [batch_size x sentence_size x embedding_size]\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    # create the complete LSTM\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "\n",
    "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
    "    outputs = final_states.h\n",
    "\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "\n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)\n",
    "\n",
    "\n",
    "lstm_classifier = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
    "                                         model_dir=os.path.join(model_dir, 'lstm'))\n",
    "train_and_evaluate(lstm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "nlp_estimators.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (Research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
