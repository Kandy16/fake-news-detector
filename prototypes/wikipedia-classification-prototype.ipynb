{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1523273466121,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "hwq4V3xwMbYk",
    "outputId": "12e9efc9-76dd-457a-de65-21b973fa8944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class                                           sentence\n",
      "0      1  Roanoke Island is an island that is part of Da...\n",
      "1      1  Roanoke was first settled around 8,000 B.C. Vi...\n",
      "2      1  At time of European Contact the island was hom...\n",
      "3      1  It was located on what is now the coast of Nor...\n",
      "4      1  Walter Raleigh and others lived there from 158...\n",
      "5      1  It was eventually given up, and nobody knows why.\n",
      "6      1  Virginia Dare, the first English child born in...\n",
      "7      1  There was a Civil War battle fought on this is...\n",
      "8      1  Union forces had the advantage because they ou...\n",
      "9      1  The Confederate Army beveling that the swamps ...\n"
     ]
    }
   ],
   "source": [
    "wiki_read = pd.read_csv('simple-wikipedia-sentences-10-article.csv')\n",
    "print(wiki_read.head(10))\n",
    "\n",
    "vocabuloryFile = open('simple-engish-wiki-10-article-vocabulory.txt','r')\n",
    "fileContent = vocabuloryFile.read()\n",
    "vocabulory_read = fileContent.split('\\n')\n",
    "#print(vocabulory_read[0])  \n",
    "vocabuloryFile.close()\n",
    "\n",
    "vocab_to_id_dict = dict(zip(vocabulory_read, range(len(vocabulory_read))))\n",
    "#print(vocab_to_id_dict)\n",
    "id_to_vocab_dict = dict(zip(range(len(vocabulory_read)), vocabulory_read))\n",
    "#print(id_to_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1362, 2423, 3422, 1646, 3274, 4409, 1296, 4020, 3192, 423, 371, 3274, 4409, 4379, 759, 4281, 3065, 3373])\n",
      " list([1362, 2423, 3591, 4020, 3274, 4347, 3305, 3274, 4305])\n",
      " list([2402, 1373, 23, 3069, 1203, 3906, 3274, 1273, 3397, 2765, 3290, 771, 1373, 1698, 3956, 3569])\n",
      " list([3130, 3274, 1655, 3274, 1441, 3397, 1373, 2383])\n",
      " list([1985, 2765, 3274, 244, 4419, 246, 826, 1154, 3192, 2318, 1211, 122, 2124, 3065, 1441, 4113])\n",
      " list([3121, 604, 3274, 4050, 2765, 4408, 1373, 3481, 1698, 3956, 2204, 1211, 1530, 551, 3274, 2094, 2765, 1028, 4116])\n",
      " list([4408, 2412, 3823, 4433, 4029, 290, 3054, 3627, 2701, 1856, 551, 1942, 1848, 721, 1028, 17, 539, 2541, 3790])\n",
      " list([4408, 3163, 3274, 4457, 4050, 4020, 3274, 4347, 3956, 3274, 1777, 2514, 2765, 44, 2540])\n",
      " list([3273, 3274, 1122, 2765, 3274, 4347, 3194, 3163, 4029, 4050, 1544, 2499, 3055, 3163, 2976, 1266, 3571, 3881])\n",
      " list([2513, 1848, 1721, 659, 2339, 3274, 1352, 81, 2412, 717, 3274, 1030, 810, 12, 2169, 2736, 721, 3274, 3891, 3956, 3672])\n",
      " list([2123, 580, 4395, 2205, 811, 3707, 1199, 2703, 3740, 2758, 2305, 1373, 3274, 2412, 484, 2133, 1161, 470, 1471, 1812, 2765, 2470, 488])\n",
      " list([2205, 399, 3274, 249, 485, 3048, 711, 3382, 1947, 551, 3274, 2799, 495, 721, 3602])\n",
      " list([4274, 1373, 3274, 2412, 3290, 3325, 3065, 1686, 2970, 721, 3274, 1957, 2330, 2176, 3274, 3950])\n",
      " list([4274, 2406, 4029, 3835, 1117, 721, 2182, 4423, 3552, 3065, 3241, 238, 721, 2884, 3769])\n",
      " list([1218, 3065, 12, 1824, 231, 4039, 1247, 2970, 721, 3666, 2330, 252, 3275, 2385])\n",
      " list([1314, 4216, 3956, 4386, 2171, 3274, 3914, 289, 2765, 1710])\n",
      " list([4148, 3274, 403, 869, 289, 1373, 2709, 2765, 2561, 4132, 512, 2765, 3982, 1000])\n",
      " list([2205, 3956, 2363, 1425, 3289, 4029, 2077, 3065, 3274, 2682, 197, 3862])\n",
      " list([1891, 2701, 2363, 4068, 4268, 2205, 2406, 4423, 1013, 2352, 249, 4303])\n",
      " list([2676, 4325, 2558, 3382, 1926, 721, 3275, 4207, 4178, 3956, 2021, 721, 3274, 201, 2744, 4207, 2748])\n",
      " list([4274, 1373, 2593, 252, 3275, 1255, 1395, 2337, 2701, 3662, 3065, 228])\n",
      " list([2205, 2731, 3274, 2412, 2254, 1705, 2765, 3274, 1666, 2598, 721, 3602])\n",
      " list([4274, 3615, 2492, 3274, 1424, 1161, 4322, 832, 647, 2598, 721, 193])\n",
      " list([4274, 1373, 3274, 2412, 3290, 1812, 3065, 2698, 869, 1209])\n",
      " list([2205, 1373, 3565, 3120, 3274, 470, 323, 2765, 394, 721, 3346])\n",
      " list([3121, 1907, 2133, 1161, 470, 2538, 2363, 311, 4106, 1439, 3088, 812, 1957, 3545, 2361])\n",
      " list([2205, 1373, 3615, 4418, 252, 2363, 562, 2354, 2765, 3769])\n",
      " list([4274, 1373, 3274, 2412, 484, 702, 1701, 721, 2133, 1161, 54])\n",
      " list([4274, 1373, 3615, 3274, 2412, 484, 1149, 2765, 4029, 1957, 3982, 1502])\n",
      " list([3121, 3274, 3216, 3382, 2021, 2793, 3274, 772, 1424, 3875, 4423, 1400, 1216, 1093, 1776, 721, 664, 3458, 1178])\n",
      " list([3121, 2010, 2765, 2363, 2689, 4020, 3956, 89, 3274, 513, 2205, 1373, 965, 3274, 210, 4065, 2765, 772, 3956, 3274, 202, 1256, 4065, 2339, 2363, 4099])\n",
      " list([2205, 1373, 3285, 4020, 4415, 3707, 1783])\n",
      " list([1314, 1115, 4332, 3592, 721, 3712, 3546])\n",
      " list([4274, 1373, 3285, 23, 4029, 3809, 2210, 3956, 2431, 2795])\n",
      " list([4274, 1373, 3274, 3155, 2765, 2410, 2308, 2339, 1058, 1951, 4320, 2178, 4291, 234, 3956, 3893, 3611])\n",
      " list([1314, 4161, 2067, 1373, 721, 2010, 2765, 1786, 3157, 4299, 3855, 81, 3499, 3256, 815, 1634, 2205, 1373, 3193])\n",
      " list([3130, 844, 3258, 1532, 3274, 1115, 721, 716, 4058, 2417, 3065, 3513, 2064])\n",
      " list([458, 2205, 1115, 2527, 4020, 4029, 6, 551, 3112, 290, 4033, 1404, 1628, 30, 566, 721, 2637])\n",
      " list([844, 2358, 2847, 1485, 2044, 1116, 3065, 2127, 3274, 2452])\n",
      " list([2205, 3823, 3793, 1405, 3372, 721, 4029, 1653, 1133, 3547])\n",
      " list([999, 2765, 4371, 2205, 3956, 2363, 2663, 2974, 4332, 1532, 617, 2765, 2561, 2311, 1893])\n",
      " list([59, 4029, 1355, 2205, 3581, 4029, 3384, 2504])\n",
      " list([3865, 2363, 1106, 130, 3203, 1818, 514, 3815, 1972])\n",
      " list([3121, 2868, 2205, 665, 2701, 3681, 3930, 3239, 2983])\n",
      " list([4274, 3422, 2875, 3473, 73, 806, 1544, 2011, 3473, 3239, 1442])\n",
      " list([4348, 12, 2205, 1373, 2352, 1404, 40, 2363, 3563, 2730, 2257, 3941, 4029, 2352, 2826, 3956, 1831, 1186, 948, 1404, 3274, 1084, 4085, 1568, 3956, 1759, 4191, 1822, 3065, 1040, 2363, 237, 721, 4205])\n",
      " list([4148, 3473, 2456, 2205, 1926, 181, 2311, 1404, 3274, 991, 117])\n",
      " list([2205, 3267, 721, 1881, 2765, 3730, 3593, 1682, 646, 3956, 3769])\n",
      " list([4274, 1926, 2214, 3956, 4024, 4020, 3274, 249, 1565, 1450, 4020, 3274, 281, 1565, 3956, 988, 4020, 3274, 1060, 1103])\n",
      " list([1906, 3274, 109, 3956, 2860, 1565, 3382, 2492, 676, 721, 3274, 1551, 3582])\n",
      " list([4274, 1373, 3615, 2709, 2765, 3274, 3650, 1103])\n",
      " list([3121, 1271, 2205, 2492, 3274, 3205, 4310, 446, 284, 721, 3274, 2722, 261, 3522, 3666, 980, 1327])\n",
      " list([4274, 3615, 2492, 4029, 2736, 4020, 3274, 2145, 249, 2926, 2052, 1103])\n",
      " list([413, 3981, 670, 2476, 323, 2765, 4337, 2687, 4412, 3956, 3393, 572])\n",
      " list([3121, 4315, 4415, 1081, 3274, 3919, 3462, 1917, 1959, 12, 2205, 2702, 3112, 1203, 666, 4469, 3274, 1850, 2826, 1404, 213, 1623, 721, 3593, 1682, 646, 249, 3956, 1268])\n",
      " list([213, 2205, 2818, 3065, 2575, 3930, 1289, 3673])\n",
      " list([2951, 3382, 351, 2363, 2311, 3025, 3502, 2638, 1682, 3593, 1493, 3956, 1851])\n",
      " list([1822, 2205, 1373, 4183, 2765, 3274, 2026, 1587, 1404, 2575, 3930, 4252])\n",
      " list([1679, 3274, 281, 1565, 3382, 1926, 1450, 3956, 2505])\n",
      " list([4274, 1373, 4029, 2214, 3956, 3682, 1357, 252, 3274, 249, 1103])\n",
      " list([4274, 399, 2072, 1551, 2586, 1778, 1729, 3502, 2363, 3674, 3268])\n",
      " list([59, 1404, 3473, 530, 2430, 2110, 2765, 3348, 2938, 4332, 584])\n",
      " list([630, 2638, 281, 1404, 276, 2205, 3727, 2363, 613])\n",
      " list([4398, 2701, 869, 558, 2652, 777, 2363, 3133, 1167, 1480, 721, 3274, 3191])\n",
      " list([1583, 1480, 1404, 276, 3382, 1373, 3174, 3065, 3274, 921])\n",
      " list([4287, 4332, 4029, 656, 3594, 1909, 81, 1958, 603, 2072, 1893])\n",
      " list([3121, 581, 3382, 1373, 3174, 3065, 3274, 953, 3930, 1289, 4030, 252, 3769])\n",
      " list([4274, 1373, 3615, 2593, 4433, 3274, 3113, 4322, 832, 2174])\n",
      " list([413, 1023, 2205, 1373, 4183, 2765, 4325, 2427, 659, 3065, 3274, 718, 1126, 2765, 3274, 3558, 3956, 1706, 615, 2281, 1887])\n",
      " list([4420, 1373, 965, 3065, 2427, 4151, 4524, 4203, 3065, 3274, 2072, 3956, 306, 260, 3956, 2645, 4218, 3163, 4165, 2765, 526, 2569, 1404, 2794, 4124, 844, 1761, 551, 489, 3382, 1065, 4332, 3438])\n",
      " list([4420, 4216, 2786, 12, 558, 2980, 3793, 181, 2994, 721, 2363, 1000])\n",
      " list([1679, 4415, 1204, 581, 3382, 1373, 2553, 2339, 3382, 2690, 1826, 2363, 3290, 1106, 1605, 1079, 3957, 3502, 2953])\n",
      " list([2205, 2731, 4029, 4293, 2039, 1586])\n",
      " list([458, 4389, 2703, 2804, 551, 4132, 1677, 1486, 2660, 2205, 3956, 3594, 2703, 667, 2205, 4029, 2006, 252, 4472, 3405, 721, 3274, 1412, 2765, 3241, 2539])\n",
      " list([3136, 3274, 3552, 2765, 2363, 2794, 1835, 1759, 2205, 3787, 522, 2205, 1065, 523, 3609, 2363, 3201, 3476, 1373, 2150, 721, 4029, 176, 2129])\n",
      " list([458, 2569, 3447, 1822, 3065, 2742, 2363, 786, 3025, 1404, 3274, 2190, 2765, 3786, 388, 854, 52, 1953, 3382, 4039, 2315, 1693, 3065, 2941, 2452])\n",
      " list([3130, 3414, 2701, 2794, 721, 1094, 782, 2205, 273, 3065, 3907])\n",
      " list([2951, 3382, 1610, 3274, 718, 2412, 2826, 3065, 2698, 991, 802, 721, 1881, 3713, 1493, 1682, 3593, 3956, 1851])\n",
      " list([4274, 1373, 4183, 2765, 1881, 231, 4020, 3274, 2601, 2510, 1229, 281, 1103])\n",
      " list([458, 2508, 4332, 4439, 3498, 1638, 3503, 3956, 2353, 1488])\n",
      " list([3503, 3498, 3956, 2205, 1818, 3793, 3201, 2765, 3274, 2229, 1881, 3199, 3690])\n",
      " list([4420, 1818, 2510, 2111, 1541, 2110, 3999, 1103])\n",
      " list([3691, 4056, 2219, 558, 3613, 2363, 2476, 1835, 249, 1373, 844, 705, 4388, 1404, 3907])\n",
      " list([4274, 3850, 3975, 721, 2363, 1247, 427, 2264, 721, 2363, 2412, 3234, 3382, 2818, 1639, 3956, 3928, 2249, 1916])\n",
      " list([630, 4029, 3920, 1404, 1343, 2205, 717, 2363, 2476, 50, 3625, 2365])\n",
      " list([3721, 1373, 4029, 2510, 704, 81, 194, 844, 2311, 3025, 1404, 843])\n",
      " list([3121, 3274, 1094, 3562, 2765, 417, 1826, 2363, 1440, 3956, 4354, 3937, 2205, 1532, 2111, 1634, 3454])\n",
      " list([4274, 2169, 4029, 4054, 4433, 4423, 4502, 786, 1309, 551, 3274, 476, 1424, 2931, 3046, 505, 721, 2898, 2064])\n",
      " list([3130, 3274, 1635, 3896, 391, 3274, 894, 2205, 2774, 3065, 852, 721, 372, 3801, 3065, 2970, 2843])\n",
      " list([4274, 1926, 252, 3274, 2775, 451, 3999, 852, 1417])\n",
      " list([3130, 4029, 3710, 427, 2205, 3081, 3065, 47, 721, 1550, 3579])\n",
      " list([2951, 3382, 2673, 252, 4029, 3025, 4433, 391, 2124, 252, 3274, 388, 854, 307, 2765, 3274, 261, 3522, 2766, 3530])\n",
      " list([366, 12, 403, 2801, 3274, 1269, 3227, 4020, 1520, 4403, 2406, 1079, 528])\n",
      " list([458, 3227, 1818, 3274, 1641, 4061, 1536, 4207, 1222, 3080])\n",
      " list([4420, 2022, 844, 281, 4303])\n",
      " list([3121, 162, 2205, 1373, 1648, 3956, 2825, 3065, 4029, 4516, 1790, 2053, 3824, 721, 2258, 1743, 330])\n",
      " list([1344, 3274, 292, 2854, 2205, 3956, 181, 4132, 3290, 2423, 651, 252, 268, 3065, 4423, 3175, 1194, 3572, 2131, 3422, 1077, 1404, 2258, 163])\n",
      " list([2, 1790, 3124, 2406, 1793, 3290, 1277, 3065, 1536, 2170, 2176, 1329, 417, 3274, 3231, 2765, 2205, 3956, 2363, 611, 4332, 531, 252, 181, 985])\n",
      " list([3130, 1508, 3502, 2852, 4128, 490, 590, 1965, 142, 1404, 2258, 3177, 3956, 3274, 3214, 2765, 3537, 225, 1965, 4423, 4502, 56, 4503, 3065, 3274, 4316, 2765, 408, 3274, 53, 4332, 1031, 3120, 2834])\n",
      " list([4420, 2664, 2639, 931, 4077, 4029, 2472, 2660, 2205, 3956, 1741])\n",
      " list([2894, 3829, 4167, 2205, 1373, 2384, 4433, 4029, 3758, 1473, 721, 4415, 1279])\n",
      " list([3187, 337, 2205, 3956, 1006, 4332, 3487])\n",
      " list([3130, 1984, 2363, 2289, 2205, 1373, 2417, 3065, 2258, 1416, 2863])\n",
      " list([2951, 3382, 3581, 3274, 3643, 2114, 2112, 15, 3991])\n",
      " list([630, 1404, 2258, 1416, 2205, 3963, 3334, 2363, 655, 3815, 3065, 960, 3274, 1797, 1986, 2952, 3157, 2765, 2228, 2245, 1289, 806, 1538, 2848, 721, 742, 2121, 2863])\n",
      " list([2995, 2406, 4469, 844, 4487, 1404, 1811, 1641, 3899, 2130, 1480, 2205, 329, 2947, 2569, 721, 1329, 1911, 3089, 844, 2639, 4303])\n",
      " list([630, 3799, 252, 1731, 2765, 93, 958, 4020, 3274, 2605, 3382, 2406, 2876, 721, 3205, 1981, 2205, 4224, 4020, 4423, 1790, 3714, 551, 4029, 96, 1067, 2945])\n",
      " list([2, 3274, 1790, 2406, 2955, 1426, 400, 3714, 3885, 3274, 3714, 1798, 3407, 2205, 3065, 4473, 3065, 3274, 2124, 2765, 3274, 1925])\n",
      " list([2205, 643]) list([458, 1798, 62, 1449])\n",
      " list([3865, 2339, 3005, 3274, 3552, 2765, 3274, 3885, 3382, 4224, 3274, 2639, 2317, 81, 2169, 2205, 3120, 2879])\n",
      " list([2205, 2652, 1966, 3065, 3274, 3115, 2082, 348, 2912, 2016, 3502, 3274, 2082, 3956, 2363, 1434])\n",
      " list([458, 2082, 3422, 4495, 2205, 3613, 747])\n",
      " list([3130, 844, 4379, 721, 3274, 4175, 553, 3544, 2918, 4526, 3065, 4296, 869, 2828, 2458, 2205, 1373, 2417, 3065, 3274, 506, 3991])\n",
      " list([458, 4379, 3194, 4263, 1793, 2205, 3065, 3613, 2494, 551, 181, 3996, 4089, 3609, 4132, 143, 961, 719, 2703, 4021, 4056, 2205, 3336, 2976, 3528])\n",
      " list([366, 3274, 1516, 2765, 3274, 2844, 721, 3353, 3460, 3274, 2090, 1826, 2205, 2406, 4469, 378, 3065, 3112, 3114, 2765, 2208, 2776, 1826, 3967, 721, 3696, 2765, 4286, 23, 4421])\n",
      " list([2205, 1373, 335, 3502, 4423, 2861, 4338, 2765, 165, 3296])\n",
      " list([2, 2363, 1786, 3765, 3274, 3643, 15, 433, 1610, 3274, 2412, 3290, 2838, 3824, 3065, 191, 380, 721, 4207, 1222, 1834, 844, 2844, 3089, 514, 2701, 727, 551, 2344])\n",
      " list([4274, 333, 3070, 1248, 1436, 721, 3274, 4305])\n",
      " list([3130, 2363, 3182, 3382, 1373, 2417, 3065, 1519, 3455, 3469])\n",
      " list([2951, 3382, 1665, 4433, 4029, 2582, 252, 1523, 3992, 3305, 1984, 4423, 248, 3092, 721, 1829, 3493])\n",
      " list([630, 1882, 2205, 717, 4423, 2982, 252, 3274, 1933, 4284, 1052, 2765, 3274, 3666, 3982, 3530])\n",
      " list([458, 1812, 221, 2205, 3065, 2376, 3274, 1052, 3956, 1046, 252, 4029, 3132])\n",
      " list([2205, 2169, 3274, 2633, 4135, 3956, 1959, 4144, 4470, 1938, 1642])\n",
      " list([3130, 3382, 1532, 3274, 14, 2205, 3081, 3065, 2363, 743, 281, 2691, 3274, 388, 854, 307, 252, 4029, 3710, 1246])\n",
      " list([2205, 3422, 2169, 4423, 3272, 2701, 2363, 743, 1106, 3956, 4487, 1797, 1986, 2995, 3065, 3613, 3274, 786, 1309, 1404, 2228, 2245, 1289, 721, 4528])\n",
      " list([458, 2072, 1373, 2709, 2765, 3274, 981, 83, 3375, 1404, 3274, 1246])\n",
      " list([458, 4054, 670, 631, 3274, 718, 1060, 3981, 252, 3274, 3789, 3596])\n",
      " list([59, 4029, 1680, 2230, 1970, 2427, 2673, 617, 252, 3274, 1060, 1103])\n",
      " list([2205, 4021, 2406, 3065, 2970, 721, 837, 2765, 3274, 1595, 3315])\n",
      " list([2, 2363, 3008, 4332, 3210, 3502, 1307, 2205, 1373, 3903, 4433, 4029, 2582, 81, 1818, 2363, 1285, 3844, 3122, 3908])\n",
      " list([4274, 2731, 3274, 3551, 3344, 3609, 3253, 2392, 2190, 1060, 1812, 88, 2747, 4029, 2476, 966, 2765, 3274, 2671, 2413])\n",
      " list([3121, 3785, 2320, 1480, 2205, 1373, 1404, 2228, 2245, 2338, 3274, 1933, 4284, 1052, 431, 514, 4029, 2743, 3272, 3065, 2970, 2884, 249, 721, 3274, 3666, 2996])\n",
      " list([2205, 1031, 4029, 823, 252, 2851, 3262, 721, 4301, 1394, 1108, 785])\n",
      " list([4420, 1373, 4029, 3835, 1238, 252, 514, 1404, 3274, 1246])\n",
      " list([4274, 1926, 3441, 252, 3274, 158, 3568, 2205, 1373, 3405, 551, 3274, 1475])\n",
      " list([4274, 2406, 2336, 3334, 3065, 3653, 4029, 2992, 1480, 2638, 721, 1515])\n",
      " list([458, 3666, 2839, 2371, 2765, 3345, 3956, 2432, 2765, 1249, 3249, 4007, 926])\n",
      " list([458, 1102, 3049, 3615, 188, 1561, 4020, 2363, 3810, 551, 2365])\n",
      " list([458, 3112, 4039, 3882, 4159, 1247, 3502, 1002])\n",
      " list([3121, 4105, 2205, 1926, 1820, 4326, 1404, 2214, 252, 3274, 25])\n",
      " list([4274, 3850, 3585, 551, 2410, 3694, 4107, 3956, 2406, 1859, 1388, 1525])\n",
      " list([4274, 3615, 1926, 721, 3274, 774, 3666, 1161, 1395, 3313, 3276, 3382, 2406, 760, 875, 721, 2410, 1124, 3274, 427, 2205, 2673, 252, 4029, 1677, 1957, 3545, 909])\n",
      " list([458, 493, 758, 4016, 1729, 4029, 440, 1404, 4146, 1952, 252, 2205, 3956, 4132, 3290, 1285, 4020, 1490, 3917, 2259])\n",
      " list([458, 907, 2801, 1373, 4423, 2576, 1729, 585, 3065, 4455, 3665, 493, 4284, 37, 2239, 3156, 3464])\n",
      " list([3691, 551, 3274, 1300, 75, 3065, 3683, 2205, 1373, 1819, 3065, 3241, 1522])\n",
      " list([2205, 1532, 3274, 440, 3637])\n",
      " list([2194, 657, 3413, 1203, 232, 721, 1329, 4359, 3274, 758, 4016, 1610, 3274, 336, 1957, 3545, 3981, 3065, 3116, 2955, 3001])\n",
      " list([3067, 3301, 2801, 2406, 2180, 3664, 237, 721, 16, 4029, 3290, 1286])\n",
      " list([3121, 3274, 51, 1799, 2103, 2788, 3603, 3956, 339, 1113, 2765, 3274, 2799, 2348, 2201, 3065, 1675, 3274, 3666, 2330, 252, 4029, 1677, 2542, 3065, 3274, 201, 3001])\n",
      " list([1660, 2593, 2205, 2701, 4029, 2950, 2765, 484, 3690])\n",
      " list([4274, 2282, 2205, 252, 1677, 2520, 3065, 1929, 3087, 1161, 579, 2691, 3274, 706, 4330])\n",
      " list([1660, 1373, 3416, 3709, 721, 3752, 497, 2363, 4251, 3215, 4039, 1836, 3793, 551, 3274, 3241, 60, 12, 3382, 558, 1027])\n",
      " list([3121, 4029, 4335, 3228, 254, 4020, 3353, 2034, 2320, 1660, 1020, 2205, 2591, 3382, 4039, 1412, 3274, 3241, 2396, 4181, 2272, 4327])\n",
      " list([4420, 1373, 4029, 195, 465, 2765, 844, 728, 386, 551, 2812, 2789, 2263, 1404, 2794, 3956, 721, 3274, 3191])\n",
      " list([2205, 1373, 2475, 1367, 97, 3904, 252, 4029, 3666, 81, 3163, 4505, 3065, 1243, 2809])\n",
      " list([1660, 1753, 12, 3382, 3588, 4029, 3666, 1812, 4475, 3962, 4104, 2976, 3065, 1243, 3117])\n",
      " list([3130, 1984, 4029, 2850, 2701, 2205, 3065, 3993, 3274, 4132, 3399, 3065, 3241, 1749, 1660, 3050, 3065, 1431, 514, 3065, 4029, 823, 252, 1496, 4029, 785])\n",
      " list([4274, 1818, 2205, 3432, 3274, 3695, 4029, 3058, 252, 3274, 1516, 2770])\n",
      " list([1660, 3684, 3065, 4402, 16, 2205, 1634, 1829, 3361, 2259])\n",
      " list([1679, 3740, 2099, 2219, 1373, 3027, 12, 2205, 558, 3613, 2825, 3065, 3274, 3570, 252, 3274, 219, 3596])\n",
      " list([1679, 3274, 780, 3131, 551, 2263, 2765, 3274, 3570, 3956, 495, 3072, 2205, 767, 2363, 823, 551, 3274, 4330])\n",
      " list([3121, 3979, 1373, 2652, 744, 3065, 4433, 1339, 2151, 3564, 2205, 1373, 3274, 2412, 3290, 249, 1812, 721, 3274, 3087, 1161, 2176, 3274, 3950])\n",
      " list([2205, 1373, 2976, 3366, 3274, 2026, 1812, 721, 3274, 3666, 2996])\n",
      " list([4279, 1285, 4112, 3320, 3956, 668, 225, 4332, 3405, 711, 2205, 1373, 2593, 2528])\n",
      " list([922, 3272, 1793, 2205, 3065, 3815, 3274, 1052, 3956, 3192, 1758, 3714, 845, 3415])\n",
      " list([4274, 2818, 3694, 3065, 2637])\n",
      " list([413, 2479, 3382, 767, 551, 3994, 1598, 1933, 4284, 4330])\n",
      " list([4420, 1373, 4029, 2283, 215, 3981, 721, 3274, 47, 3433, 3530])\n",
      " list([2915, 12, 698, 3382, 3047, 2222, 1260, 551, 2578, 1103])\n",
      " list([1314, 2390, 1006, 2847, 4433, 4029, 3243, 721, 3458, 1491, 4284, 1480, 3382, 1373, 2518])\n",
      " list([1679, 2662, 3319, 4476, 2205, 3956, 1006, 4332, 1460, 3502, 3192, 743, 2301, 1797, 1986, 532])\n",
      " list([3121, 4476, 2205, 1125, 1404, 2424, 1943, 618, 252, 1094, 1145, 551, 3274, 706, 3570, 2765, 3274, 119, 905, 3087, 3530])\n",
      " list([844, 1605, 3194, 3405, 489, 721, 451, 2387, 1177])\n",
      " list([4274, 1373, 2976, 1793, 3065, 2050, 551, 2363, 2938, 1404, 3274, 3981, 720])\n",
      " list([3942, 3382, 2527, 1404, 3274, 3694, 2765, 4029, 879, 3290, 3063])\n",
      " list([4095, 3274, 495, 3981, 3336, 2976, 1426, 4029, 1094, 1145, 3639, 3274, 3049, 1373, 1844, 3502, 3274, 1848, 721, 3274, 2428])\n",
      " list([1985, 2765, 3069, 1848, 3336, 2976, 4296, 1973, 2569, 2738, 2205, 1211, 4022, 2199, 2578, 3290, 1812, 522, 1660, 2406, 767, 3065, 3274, 495, 721, 1406])\n",
      " list([3121, 1572, 618, 3274, 3594, 2435, 4217, 3382, 558, 1495, 4326, 2591, 2205, 3956, 929, 3336, 2976, 1280, 1145, 1941])\n",
      " list([999, 2765, 4371, 2205, 1373, 431, 2124, 3065, 2424, 2166])\n",
      " list([3121, 1695, 3274, 2331, 1373, 173, 4181, 2916, 4020, 3234, 1668])\n",
      " list([4420, 1373, 3407, 3502, 3274, 4390, 1773, 3956, 4506, 827, 1107])\n",
      " list([3121, 2841, 4029, 79, 3234, 1373, 2296, 2217, 465, 2765, 346, 4394, 4198])\n",
      " list([3130, 4029, 2077, 2765, 3235, 3065, 879, 2263, 3502, 2103, 3274, 3570, 4332, 1793, 3065, 4212, 4029, 3234, 2738, 2205, 721, 2424, 2166])\n",
      " list([2205, 1818, 2363, 3570, 3071, 1404, 2424, 2078, 4284, 826, 862, 4020, 1878, 3861, 302])\n",
      " list([2402, 1373, 4423, 2802, 3234, 1826, 3274, 2827])\n",
      " list([1906, 3274, 3234, 2205, 1610, 3274, 2412, 484, 3065, 1686, 2970, 252, 4029, 217, 3545, 3981, 3956, 1826, 4029, 1957, 3545, 3981, 2176, 3274, 249, 485, 3934, 2406, 4469, 1836, 721, 2736, 721, 3274, 3950])\n",
      " list([2915, 721, 1094, 2290, 2339, 837, 1405, 3372, 2156, 2205, 1373, 2417, 2701, 2214, 3065, 3758, 4401])\n",
      " list([4420, 1793, 514, 3065, 4455, 55, 510, 3065, 2412, 4401])\n",
      " list([844, 1130, 4200, 4521])]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1]\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (831, 25)\n",
      "x_test shape: (190, 25)\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "train_labels = []\n",
    "for index, row in wiki_read.iterrows():\n",
    "    train_features.append([vocab_to_id_dict[word] for word in row['sentence'].split(' ')])\n",
    "    train_labels.append(row['class'])\n",
    "\n",
    "test_features = train_features[10:200]\n",
    "test_labels = train_labels[10:200]\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(test_features)\n",
    "print(test_labels)\n",
    "\n",
    "vocab_size = len(vocabulory_read)\n",
    "sentence_size = 25\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# we assign the first indices in the vocabulary to special tokens that we use\n",
    "# for padding, as start token, and for indicating unknown words\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(train_features, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "y_train = train_labels\n",
    "x_test = sequence.pad_sequences(test_features, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "\n",
    "y_test = test_labels\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VPLRuDRnvq92"
   },
   "outputs": [],
   "source": [
    "x_len_train = np.array([min(len(x), sentence_size) for x in train_features])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in test_features])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_features))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NMHErNXuBFHr"
   },
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    # Save a reference to the classifier to run predictions later\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=2500)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()\n",
    "#     # Un-comment code to download experiment data from Colaboratory\n",
    "#     from google.colab import files\n",
    "#     model_name = os.path.basename(os.path.normpath(classifier.model_dir))\n",
    "#     ! zip -r {model_name + '.zip'} {classifier.model_dir}\n",
    "#     files.download(model_name + '.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 215179,
     "status": "ok",
     "timestamp": 1520289526441,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "QSNT9KXtt_Up",
    "outputId": "dfbaee5e-3b3d-4c2a-db64-9b3350acdf65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpj6ka397k/lstm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4c44379780>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpj6ka397k/lstm/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.166145, step = 1\n",
      "INFO:tensorflow:global_step/sec: 9.61782\n",
      "INFO:tensorflow:loss = 8.005314, step = 101 (10.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.12\n",
      "INFO:tensorflow:loss = 0.9029907, step = 201 (9.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0151\n",
      "INFO:tensorflow:loss = 0.26449886, step = 301 (9.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.82303\n",
      "INFO:tensorflow:loss = 0.27883467, step = 401 (10.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.82028\n",
      "INFO:tensorflow:loss = 0.112120256, step = 501 (10.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.84533\n",
      "INFO:tensorflow:loss = 0.12684532, step = 601 (10.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.82496\n",
      "INFO:tensorflow:loss = 0.02731225, step = 701 (10.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.85541\n",
      "INFO:tensorflow:loss = 0.0044986797, step = 801 (10.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.6542\n",
      "INFO:tensorflow:loss = 0.09018436, step = 901 (10.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.72131\n",
      "INFO:tensorflow:loss = 0.007908237, step = 1001 (10.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.72916\n",
      "INFO:tensorflow:loss = 0.024489187, step = 1101 (10.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.6016\n",
      "INFO:tensorflow:loss = 0.010881645, step = 1201 (10.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.48037\n",
      "INFO:tensorflow:loss = 0.040963333, step = 1301 (10.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.11804\n",
      "INFO:tensorflow:loss = 0.0041306345, step = 1401 (10.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.73506\n",
      "INFO:tensorflow:loss = 0.007006402, step = 1501 (10.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.66025\n",
      "INFO:tensorflow:loss = 0.009553065, step = 1601 (10.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.27707\n",
      "INFO:tensorflow:loss = 0.0009083056, step = 1701 (10.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.65961\n",
      "INFO:tensorflow:loss = 0.018544158, step = 1801 (10.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.82323\n",
      "INFO:tensorflow:loss = 0.005225982, step = 1901 (10.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.57526\n",
      "INFO:tensorflow:loss = 0.0031345407, step = 2001 (10.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.78081\n",
      "INFO:tensorflow:loss = 0.0025892337, step = 2101 (10.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.79694\n",
      "INFO:tensorflow:loss = 0.002966656, step = 2201 (10.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.7796\n",
      "INFO:tensorflow:loss = 0.0025908267, step = 2301 (10.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.98923\n",
      "INFO:tensorflow:loss = 0.0019296703, step = 2401 (11.133 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /tmp/tmpj6ka397k/lstm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00387436.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-31-10:50:52\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpj6ka397k/lstm/model.ckpt-2500\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-31-10:50:53\n",
      "INFO:tensorflow:Saving dict for global step 2500: accuracy = 1.0, accuracy_baseline = 1.0, auc = 0.0, auc_precision_recall = 1.0, average_loss = 1.4905124e-05, global_step = 2500, label/mean = 1.0, loss = 0.0014159868, prediction/mean = 0.99998504\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpj6ka397k/lstm/model.ckpt-2500\n"
     ]
    }
   ],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def lstm_model_fn(features, labels, mode):    \n",
    "    # [batch_size x sentence_size x embedding_size]\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    # create the complete LSTM\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "\n",
    "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
    "    outputs = final_states.h\n",
    "\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "\n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)\n",
    "\n",
    "\n",
    "lstm_classifier = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
    "                                         model_dir=os.path.join(model_dir, 'lstm'))\n",
    "train_and_evaluate(lstm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "nlp_estimators.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (Research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
